{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Extraction with Textacy\n",
    "\n",
    "## Setup\n",
    "For most uses of textacy, language-specific model data for spacy must first be downloaded. Follow the directions [here](https://spacy.io/docs/usage/models).\n",
    "\n",
    "```bash\n",
    "$ pip install textacy\n",
    "```\n",
    "\n",
    "Experiments with semi-supervised and unsupervised learning for entity clustering were unimpressive and not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import random\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "import csv\n",
    "with open('data/comments.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    comments = [comment for comment in reader]\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to improve performance for development, take a random sample of 25k comments\n",
    "# comments = random.sample(comments, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('documentId', 'DOI-2017-0002-0002'),\n",
       "              ('postedDate', '2017-05-11T00:00:00-04:00'),\n",
       "              ('attachmentCount', '0'),\n",
       "              ('commentText',\n",
       "               'Our national monuments are a national treasure for all to enjoy. Leave them alone!')]),\n",
       " OrderedDict([('documentId', 'DOI-2017-0002-0003'),\n",
       "              ('postedDate', '2017-05-11T00:00:00-04:00'),\n",
       "              ('attachmentCount', '0'),\n",
       "              ('commentText',\n",
       "               '1.\\tWe do not want National Monument protection removed for ANY of the 27 National Monuments under review.\\n2.\\tStates and local residents do not have an inherent right to use federal land as they wish.  Control of this land does not lie with the states and the local population.\\n3.\\tThe Antiquities Act, under which the monuments were designated, does not give the president explicit power to undo a designation and no president has ever taken such a step.\\n4.\\tWeakening existing protections for these monuments will likely result in development and exploitation, ruining or de-valuing the values (natural and cultural) for which they were designated. \\n5.\\tThe monuments to be reviewed boast 1.9 million acres of canyons, arches, and pion-juniper forests. They house artifacts, buildings, and sacred spaces of Native American communities.\\n')])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SpaCy\n",
    "Define rules for the monument names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "from spacy.attrs import IS_PUNCT, LOWER, ORTH, IS_SPACE\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "def merge_phrase(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    # print(matches[i])\n",
    "    if i != len(matches)-1: # if not the last match, keep going\n",
    "        return None\n",
    "\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "\n",
    "    for ent_id, label, span in spans:\n",
    "\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_, ent_id = ent_id)\n",
    "\n",
    "    \n",
    "#         for eid in set([ent_id for ent_id, label, start, end in matches]):\n",
    "#         # for each entity id, find the \n",
    "#     x = 0\n",
    "#     for x, span in enumerate(spans):\n",
    "#         if x < len(spans)-1 and spans[x+1][0] != span[0]: # next entity is not the same\n",
    "#             span[2].merge(ent_id=span[0], label=span[1])\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "nlp.matcher.add_entity('BasinandRange', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'BasinandRange',\n",
    "[{66: 'basin'},\n",
    " {66: 'and', 'OP': '?'},\n",
    " {66: 'range'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('BearsEars', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'BearsEars',\n",
    "[{66: 'bears'},\n",
    " {66: 'ears'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'BearsEars',\n",
    "[{66: 'bears'},\n",
    " {66: 'ears'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('BerryessaSnowMountain', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'BerryessaSnowMountain',\n",
    "[{66: 'berryessa'},\n",
    " {66: 'snow'},\n",
    " {66: 'mountain'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'BerryessaSnowMountain',\n",
    "[{66: 'berryessa'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'BerryessaSnowMountain',\n",
    "[{66: 'snow'},\n",
    " {66: 'mountain'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('CanyonsoftheAncients', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'CanyonsoftheAncients',\n",
    "[{66: 'canyons'},\n",
    " {66: 'of', 'OP': '?'},\n",
    " {66: 'the', 'OP': '?'},\n",
    " {66: 'ancients'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'CanyonsoftheAncients',\n",
    "[{66: 'canyon'},\n",
    " {66: 'of', 'OP': '?'},\n",
    " {66: 'the', 'OP': '?'},\n",
    " {66: 'ancients'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_entity('CarrizoPlain', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'CarrizoPlain',\n",
    "[{66: 'carrizo'},\n",
    " {66: 'plain'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('CascadeSiskiyou', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'CascadeSiskiyou',\n",
    "[{66: 'cascade', 'OP': '?'},\n",
    " {66: 'siskiyou'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('CratersoftheMoon', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'CratersoftheMoon',\n",
    "[{66: 'craters'},\n",
    " {66: 'of'},\n",
    " {66: 'the'},\n",
    " {66: 'moon'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'CratersoftheMoon',\n",
    "[{66: 'craters', 'OP': '?'},\n",
    " {66: 'of', 'OP': '?'},\n",
    " {66: 'the', 'OP': '?'},\n",
    " {66: 'moon'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "\n",
    "nlp.matcher.add_entity('GiantSequoia', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'GiantSequoia',\n",
    "[{66: 'giant', 'OP': '?'},\n",
    " {66: 'sequoia'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('GoldButte', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'GoldButte',\n",
    "[{66: 'gold'},\n",
    " {66: 'butte'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('GrandCanyonParashant', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandCanyonParashant',\n",
    "[{66: 'grand', 'OP': '?'},\n",
    " {66: 'canyon', 'OP': '?'},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    " {IS_PUNCT: True, 'OP': '*'},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    " {66: 'parashant',}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('GrandStaircaseEscalante', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandStaircaseEscalante',\n",
    "[{66: 'grand', 'OP': '?'},\n",
    " {66: 'staircase', 'OP': '?'},\n",
    " {IS_SPACE: True, 'OP': '*'},\n",
    " {IS_PUNCT: True},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    " {66: 'escalante'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandStaircaseEscalante',\n",
    "[{66: 'grand'},\n",
    " {66: 'staircase'},\n",
    " {66: 'escalante', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandStaircaseEscalante',\n",
    "[{66: 'escalante'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandStaircaseEscalante',\n",
    "[{66: 'staircase'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('HanfordReach', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'HanfordReach',\n",
    "[{66: 'hanford'},\n",
    " {66: 'reach'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('IronwoodForest', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'IronwoodForest',\n",
    "[{66: 'ironwood'},\n",
    " {66: 'forest'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('MojaveTrails', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'MojaveTrails',\n",
    "[{66: 'mojave'},\n",
    " {66: 'trails'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('OrganMountainsDesertPeaks', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'OrganMountainsDesertPeaks',\n",
    "[{66: 'organ', 'OP': '?'},\n",
    " {66: 'mountains', 'OP': '?'},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    "  {IS_PUNCT: True},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    " {66: 'desert', 'OP': '?'},\n",
    " {66: 'peaks'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'OrganMountainsDesertPeaks',\n",
    "[{66: 'organ'},\n",
    " {66: 'mountains', 'OP': '?'},\n",
    " {66: 'desert', 'OP': '?'},\n",
    " {66: 'peaks', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('RioGrandedelNorte', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'RioGrandedelNorte',\n",
    "[{66: 'rio', 'OP': '?'},\n",
    " {66: 'grande', 'OP': '?'},\n",
    " {66: 'del', 'OP': '?'},\n",
    " {66: 'norte'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'RioGrandedelNorte',\n",
    "[{66: 'rio'},\n",
    " {66: 'grande'},\n",
    " {66: 'del', 'OP': '?'},\n",
    " {66: 'norte', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('SandtoSnow', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'SandtoSnow',\n",
    "[{66: 'sand', 'OP': '?'},\n",
    " {66: 'to', 'OP': '?'},\n",
    " {66: 'snow'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('SanGabrielMountains', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'SanGabrielMountains',\n",
    "[{66: 'san', 'OP': '?'},\n",
    " {66: 'gabriel'},\n",
    " {66: 'mountains', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('SonoranDesert', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'SonoranDesert',\n",
    "[{66: 'sonoran'},\n",
    " {66: 'desert', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('UpperMissouriRiverBreaks', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'UpperMissouriRiverBreaks',\n",
    "[{66: 'upper', 'OP': '?'},\n",
    " {66: 'missouri'},\n",
    " {66: 'river', 'OP': '?'},\n",
    " {66: 'breaks', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('VermilionCliffs', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'VermilionCliffs',\n",
    "[{66: 'vermilion'},\n",
    " {66: 'cliffs'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('KatahdinWoodsandWaters', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'KatahdinWoodsandWaters',\n",
    "[{66: 'katahdin'},\n",
    " {66: 'woods', 'OP': '?'},\n",
    " {66: 'and', 'OP': '?'},\n",
    " {66: 'waters', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'KatahdinWoodsandWaters',\n",
    "[{66: 'katahdin'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('MarianasTrenchMarine', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'MarianasTrenchMarine',\n",
    "[{66: 'marianas'},\n",
    " {66: 'trench', 'OP': '?'},\n",
    " {66: 'marine', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('Seamounts', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'Seamounts',\n",
    "[{66: 'northeast'},\n",
    " {66: 'canyons'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'Seamounts',\n",
    "[{66: 'seamounts'},\n",
    "  {66: 'marine', 'OP': '?'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('PacificRemoteIslandsMarine', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'PacificRemoteIslandsMarine',\n",
    "[{66: 'pacific'},\n",
    " {66: 'remote', 'OP': '?'},\n",
    " {66: 'islands'},\n",
    " {66: 'marine', 'OP': '?'},\n",
    "{66: 'national', 'OP': '?'},\n",
    "{66: 'monument', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('PapahanaumokuakeaMarine', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'PapahanaumokuakeaMarine',\n",
    "[{66: 'papahanaumokuakea'},\n",
    " {66: 'marine', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('RoseAtollMarine', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'RoseAtollMarine',\n",
    "[{66: 'rose'},\n",
    " {66: 'atoll', 'OP': '?'},\n",
    " {66: 'marine', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.pipeline = [nlp.tagger, nlp.entity, nlp.matcher, nlp.parser]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some testing\n",
    "The cell below is just used for testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mons = \"\"\"Basin and Range National Monument\n",
    "Bears Ears National Monument\n",
    "Berryessa Snow Mountain\n",
    "Canyons of the Ancients\n",
    "Carrizo Plain National Monument\n",
    "Cascade Siskiyou\n",
    "Craters of the Moon National Monument\n",
    "craters of the moon monument\n",
    "Giant Sequoia\n",
    "Gold Butte\n",
    "Grand Canyon-Parashant\n",
    "Grand Canyon / Parashant National Monument\n",
    "Grand Staircase-Escalante\n",
    "Hanford Reach\n",
    "Ironwood Forest\n",
    "Mojave Trails\n",
    "Organ Mountains-Desert Peaks\n",
    "Rio Grande del Norte\n",
    "Sand to Snow\n",
    "San Gabriel Mountains\n",
    "Sonoran Desert\n",
    "Upper Missouri River Breaks\n",
    "Vermilion Cliffs\n",
    "Katahdin Woods and Waters California\n",
    "Marianas Trench Marine\n",
    "Pacific Remote Islands Marine National Monument\n",
    "Papahanaumokuakea Marine\n",
    "Rose Atoll Marine\"\"\"\n",
    "for mon in mons.split('\\n'):\n",
    "#     entity_name = ((mon.replace(' ', '')).replace('-', '')).replace(r'\\n', '')\n",
    "#     print(\"matcher.add_entity('%s', on_match=merge_phrases)\" % entity_name)\n",
    "#     pattern = [{LOWER: word} for word in mon.split()]\n",
    "#     pattern.append({LOWER: 'national', 'OP': '?'})\n",
    "#     pattern.append({LOWER: 'monument', 'OP': '?'})\n",
    "#     print('matcher.add_pattern(')\n",
    "#     print(\"'%s',\" % entity_name)\n",
    "#     pprint(pattern)\n",
    "#     print(\",label='GPE'\\n)\\n\")\n",
    "    doc = nlp('blah blah ' + mon + ' blah')\n",
    "    #matches = matcher(doc)\n",
    "#    print([w.text for w in doc])\n",
    "    print([ent.orth_ for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the slow, more accurate way to build a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(422000 docs; 69155128 tokens)\n"
     ]
    }
   ],
   "source": [
    "# this is pretty inefficient--it would be better to use \n",
    "# SpaCy's Pipe function, but I kept running into vocab issues\n",
    "\n",
    "# Only run this to update--takes 30 - 45 mins. Otherwise, just read in the corpus we've saved\n",
    "\n",
    "if 1 == 1:\n",
    "# Split records’ content (text) field from associated metadata fields, \n",
    "# but keep them paired together for convenient loading into a textacy.Corpus\n",
    "    #docs = [textacy.doc.Doc(doc) for doc in nlp.pipe(text_stream, batch_size=1000, n_threads=4)]\n",
    "    #docs = [nlp(doc) for doc in text_stream]\n",
    "    \n",
    "    #corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream, itemwise = False)\n",
    "    corpus = textacy.Corpus(nlp)\n",
    "    #corpus.spacy_vocab = nlp.vocab\n",
    "    for comment in comments:\n",
    "        corpus.add_doc(nlp(comment['commentText']), metadata=comment )\n",
    "    #corpus.save('data', name='textacy-corpus', compression='gzip')\n",
    "else:\n",
    "# or, read in the corpus we've already parsed\n",
    "    corpus = textacy.Corpus.load('data', name='textacy-corpus', compression='gzip')\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the faster, less accurate way to build a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fast, but without monument matching\n",
    "text_stream, metadata_stream = textacy.fileio.split_record_fields(comments, 'comment')\n",
    "corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some cool stuff you can do with textacy\n",
    "\n",
    "# counts =\n",
    "corpus.word_freqs(normalize=True, weighting='count')\n",
    "# idf = \n",
    "corpus.word_doc_freqs(normalize=True, weighting='idf')\n",
    "list(textacy.extract.ngrams(doc, 3, filter_stops=True, filter_punct=True, filter_nums=False))[:15]\n",
    "\n",
    "doc = corpus[0]\n",
    "ts = textacy.text_stats.TextStats(doc)\n",
    "print(ts.n_unique_words)\n",
    "print(ts.basic_counts)\n",
    "print(ts.readability_stats)\n",
    "\n",
    "doc.to_bag_of_terms(named_entities=True, normalize='lemma', filter_stops=True, filter_punct=True)\n",
    "\n",
    "doc.metadata\n",
    "\n",
    "corpus.word_doc_freqs(lemmatize=True, lowercase=True, weighting='count', as_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Bears Ears National Monument', 'bears', 'GPE'), 186455),\n",
       " (('Bears Ears', 'bears', 'GPE'), 125332),\n",
       " (('', '', ''), 110295),\n",
       " (('Utah', 'utah', 'GPE'), 98895),\n",
       " (('American', 'american', 'NORP'), 93921),\n",
       " (('National Monuments', 'national monuments', 'ORG'), 71252),\n",
       " (('Antiquities Act', 'antiquities act', ''), 67262),\n",
       " (('Trump', 'trump', 'PERSON'), 57464),\n",
       " (('Zinke', 'zinke', 'PERSON'), 54801),\n",
       " (('Americans', 'americans', 'NORP'), 53441)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a count of named entities\n",
    "\n",
    "# Extract the named entities from each comment\n",
    "from collections import Counter\n",
    "c = Counter()\n",
    "for doc in corpus:\n",
    "    ents = textacy.extract.named_entities(doc, include_types=('PERSON', 'LAW', 'ORG', 'NORP', 'GPE', 'WORK_OF_ART'), drop_determiners=True)\n",
    "    for ent in ents:\n",
    "        e = (ent.orth_, ent.lemma_, ent.label_)\n",
    "        c[e] += 1\n",
    "\n",
    "c.most_common(10) \n",
    "# should I drop determiners? Probably not.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Id': 1996,\n",
       "  'frequency': 116,\n",
       "  'lemma': 'bluff ,',\n",
       "  'pos': 'GPE',\n",
       "  'value': 'Bluff,'},\n",
       " {'Id': 1997,\n",
       "  'frequency': 116,\n",
       "  'lemma': 'slickhorn gulch',\n",
       "  'pos': 'ORG',\n",
       "  'value': 'Slickhorn Gulch'},\n",
       " {'Id': 1998,\n",
       "  'frequency': 116,\n",
       "  'lemma': 'butler wash',\n",
       "  'pos': 'ORG',\n",
       "  'value': 'Butler Wash'},\n",
       " {'Id': 1999,\n",
       "  'frequency': 116,\n",
       "  'lemma': 'owl canyon',\n",
       "  'pos': 'PERSON',\n",
       "  'value': 'Owl Canyon'},\n",
       " {'Id': 2000,\n",
       "  'frequency': 116,\n",
       "  'lemma': 'dark canyon',\n",
       "  'pos': 'GPE',\n",
       "  'value': 'Dark Canyon'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents = list()\n",
    "for ent in c.most_common(2000):\n",
    "    ents.append({'value': ent[0][0],\n",
    "                'lemma': ent[0][1],\n",
    "                'pos': ent[0][2],\n",
    "                'frequency': ent[1]})\n",
    "\n",
    "for i, row in enumerate(ents):\n",
    "    ents[i][\"Id\"] = i+1\n",
    "\n",
    "entities = set([ent['value'] for ent in ents])\n",
    "ents[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Dead Horse State Park, Canyonlands National Park, Arches National Park, '\n",
      "  'Monument Valley'},\n",
      " {'National Monuments to National Parks: The Use of the Antiquities Act of'},\n",
      " {'Is the Midway Atoll National Wildlife Refuge Being Properly Managed'},\n",
      " {'\"Review of Certain National Monuments Established Since 1996',\n",
      "  'Review of Certain National Monuments Established Since 1996'},\n",
      " {'Bears Ears Monument Was the Right Decision at the Right Time'},\n",
      " {'Section 106 of the National Historic Preservation Act'},\n",
      " {'Review of Certain National Monuments',\n",
      "  'Review of Certain National Monuments Established'},\n",
      " {'Organ Mountains- Desert Peaks National Monument',\n",
      "  'Organ Mountains-Desert Peaks National Monument'},\n",
      " {'Bears Ears National Monument - Monument Review'},\n",
      " {'Katahdin Woods and Waters National Monument',\n",
      "  'Katahdin Woods and Waters National Monuments'}]\n"
     ]
    }
   ],
   "source": [
    "# group things together, just to make the lookup table building a little easier\n",
    "textacy_variants = textacy.keyterms.aggregate_term_variants(entities, fuzzy_dedupe=True)\n",
    "import pprint\n",
    "pprint.pprint(textacy_variants[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the varients. But, don't actually do this!\n",
    "# varients = [list(var) for var in textacy_variants]\n",
    "# def getKey(item):\n",
    "#     return item[0]\n",
    "# varients = sorted(varients, key=getKey)\n",
    "# varients[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a dataset with the named entities and frequencies\n",
    "# but dont actually do this\n",
    "# with open('ents.csv', 'w') as f:\n",
    "#     writer = csv.DictWriter(f, ['Id', 'value', 'lemma', 'pos', 'frequency'])\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the dataset used to create the named entity resolution stuff\n",
    "# this is the part where I painstakenly go through the resulting\n",
    "# spreadsheet and manually map instances to their canonical value\n",
    "with open('ents.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # flatten the grouped entities\n",
    "    # to do: include a count of each\n",
    "    for item in [item for sublist in textacy_variants for item in sublist]:\n",
    "        writer.writerow([item])\n",
    "# save the resulting file as data/entity-resolution.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag docs with named entities\n",
    "1. read in 'data/entity-resolution.csv'\n",
    "2. tag each document with the named entities it contains\n",
    "3. export as to csv (data/named-entities.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total named instances 1568\n",
      "unique named entities 629\n"
     ]
    }
   ],
   "source": [
    "# read entities into a list of tuples of form (instance, entity)\n",
    "with open('data/entity-resolution.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    er = [ent for ent in reader]\n",
    "er = [(item['instance'], item['canonical']) for item in er if item['canonical'] != 'ignore']\n",
    "lookup = {e[0]: e[1] for e in er}\n",
    "\n",
    "entities = set(lookup.values())\n",
    "lookup.update({e: e for e in entities})\n",
    "instances = set(lookup.keys()).union(entities)\n",
    "\n",
    "print('total named instances', str(len(instances)))\n",
    "print('unique named entities', str(len(entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe with\n",
    "# rows: document_ids\n",
    "# columns: canonical named entities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(False, index=[doc.metadata.get('documentId') for doc in corpus], columns=entities, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each doc, get list of named entities, and update df\n",
    "# setting the appropriate canonical name columns to true for matches\n",
    "from spacy.parts_of_speech import DET # to remove determiners\n",
    "for doc in corpus:\n",
    "    row_idx = doc.metadata.get('documentId')\n",
    "    nes = [ne if ne[0].pos != DET else ne[1:] for ne in doc.spacy_doc.ents]\n",
    "    nes = set([ne.orth_ for ne in nes])\n",
    "    df.loc[row_idx, [lookup[v] for v in instances.intersection(nes)]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert bool to 0/1 and write to file\n",
    "(df*1).to_csv('data/named-entities.csv', index_label='documentId',) # mult. by 1 to convert boolean to 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Ryan Erik Benally,\n",
       " Montezuma Creek,\n",
       " Utah,\n",
       " less than,\n",
       " 50 miles,\n",
       " Bears Ears National Monument,\n",
       " Rescinding,\n",
       " Bears Ears National Monument,\n",
       " Trump,\n",
       " Executive Order,\n",
       " Review of Certain National Monuments,\n",
       " 1996,\n",
       " Bears Ears National Monument,\n",
       " the Antiquities Act,\n",
       " over 100,000 acres,\n",
       " Act,\n",
       " \n",
       " \n",
       " Native,\n",
       " Local Residents,\n",
       " Rescinding,\n",
       " Bears Ears National Monument,\n",
       " Grand Staircase-Escalante,\n",
       " \n",
       " \n",
       " I,\n",
       " Trump,\n",
       " the,\n",
       " Bears Ears National Monument,\n",
       " National Monument,\n",
       " the State of Utah)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = corpus[3]\n",
    "\n",
    "doc.spacy_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columbus</th>\n",
       "      <th>Pima County</th>\n",
       "      <th>Oceanside</th>\n",
       "      <th>Alabama</th>\n",
       "      <th>Patagonia</th>\n",
       "      <th>Beverly</th>\n",
       "      <th>Miami</th>\n",
       "      <th>Olympic National Park</th>\n",
       "      <th>Cedar Breaks National Monument</th>\n",
       "      <th>Mormon Environmental Stewardship Alliance</th>\n",
       "      <th>...</th>\n",
       "      <th>Woodbridge</th>\n",
       "      <th>Richland</th>\n",
       "      <th>Dark Canyon</th>\n",
       "      <th>Comb Ridge</th>\n",
       "      <th>Paso Robles</th>\n",
       "      <th>Trout Unlimited</th>\n",
       "      <th>Bighorn Sheep</th>\n",
       "      <th>Statue of Liberty</th>\n",
       "      <th>Blanding</th>\n",
       "      <th>Yellowstone National Park</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DOI-2017-0002-0005</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOI-2017-0002-0005</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOI-2017-0002-0005</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOI-2017-0002-0005</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 629 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Columbus  Pima County  Oceanside  Alabama  Patagonia  \\\n",
       "DOI-2017-0002-0005     False        False      False    False      False   \n",
       "DOI-2017-0002-0005     False        False      False    False      False   \n",
       "DOI-2017-0002-0005     False        False      False    False      False   \n",
       "DOI-2017-0002-0005     False        False      False    False      False   \n",
       "\n",
       "                    Beverly  Miami  Olympic National Park  \\\n",
       "DOI-2017-0002-0005    False  False                  False   \n",
       "DOI-2017-0002-0005    False  False                  False   \n",
       "DOI-2017-0002-0005    False  False                  False   \n",
       "DOI-2017-0002-0005    False  False                  False   \n",
       "\n",
       "                    Cedar Breaks National Monument  \\\n",
       "DOI-2017-0002-0005                           False   \n",
       "DOI-2017-0002-0005                           False   \n",
       "DOI-2017-0002-0005                           False   \n",
       "DOI-2017-0002-0005                           False   \n",
       "\n",
       "                    Mormon Environmental Stewardship Alliance  \\\n",
       "DOI-2017-0002-0005                                      False   \n",
       "DOI-2017-0002-0005                                      False   \n",
       "DOI-2017-0002-0005                                      False   \n",
       "DOI-2017-0002-0005                                      False   \n",
       "\n",
       "                              ...              Woodbridge  Richland  \\\n",
       "DOI-2017-0002-0005            ...                   False     False   \n",
       "DOI-2017-0002-0005            ...                   False     False   \n",
       "DOI-2017-0002-0005            ...                   False     False   \n",
       "DOI-2017-0002-0005            ...                   False     False   \n",
       "\n",
       "                    Dark Canyon  Comb Ridge  Paso Robles  Trout Unlimited  \\\n",
       "DOI-2017-0002-0005        False       False        False            False   \n",
       "DOI-2017-0002-0005        False       False        False            False   \n",
       "DOI-2017-0002-0005        False       False        False            False   \n",
       "DOI-2017-0002-0005        False       False        False            False   \n",
       "\n",
       "                    Bighorn Sheep  Statue of Liberty  Blanding  \\\n",
       "DOI-2017-0002-0005          False              False     False   \n",
       "DOI-2017-0002-0005          False              False     False   \n",
       "DOI-2017-0002-0005          False              False     False   \n",
       "DOI-2017-0002-0005          False              False     False   \n",
       "\n",
       "                    Yellowstone National Park  \n",
       "DOI-2017-0002-0005                      False  \n",
       "DOI-2017-0002-0005                      False  \n",
       "DOI-2017-0002-0005                      False  \n",
       "DOI-2017-0002-0005                      False  \n",
       "\n",
       "[4 rows x 629 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['DOI-2017-0002-0005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
