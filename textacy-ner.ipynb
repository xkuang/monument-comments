{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Extraction with Textacy\n",
    "\n",
    "## Setup\n",
    "For most uses of textacy, language-specific model data for spacy must first be downloaded. Follow the directions [here](https://spacy.io/docs/usage/models).\n",
    "\n",
    "```bash\n",
    "$ pip install textacy\n",
    "```\n",
    "\n",
    "Experiments with semi-supervised and unsupervised learning for entity clustering were unimpressive and not successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import random\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251623"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "import csv\n",
    "with open('data/comments.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    comments = [comment for comment in reader]\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to improve performance for development, take a random sample of 25k comments\n",
    "# comments = random.sample(comments, 25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SpaCy\n",
    "Define rules for the monument names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "from spacy.attrs import IS_PUNCT, LOWER, ORTH, IS_SPACE\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "def merge_phrase(matcher, doc, i, matches):\n",
    "    '''\n",
    "    Merge a phrase. We have to be careful here because we'll change the token indices.\n",
    "    To avoid problems, merge all the phrases once we're called on the last match.\n",
    "    '''\n",
    "    # print(matches[i])\n",
    "    if i != len(matches)-1: # if not the last match, keep going\n",
    "        return None\n",
    "\n",
    "    # Get Span objects\n",
    "    spans = [(ent_id, label, doc[start : end]) for ent_id, label, start, end in matches]\n",
    "\n",
    "    for ent_id, label, span in spans:\n",
    "\n",
    "        span.merge(label=label, tag='NNP' if label else span.root.tag_, ent_id = ent_id)\n",
    "\n",
    "    \n",
    "#         for eid in set([ent_id for ent_id, label, start, end in matches]):\n",
    "#         # for each entity id, find the \n",
    "#     x = 0\n",
    "#     for x, span in enumerate(spans):\n",
    "#         if x < len(spans)-1 and spans[x+1][0] != span[0]: # next entity is not the same\n",
    "#             span[2].merge(ent_id=span[0], label=span[1])\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "nlp.matcher.add_entity('BasinandRange', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'BasinandRange',\n",
    "[{66: 'basin'},\n",
    " {66: 'and', 'OP': '?'},\n",
    " {66: 'range'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('BearsEars', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'BearsEars',\n",
    "[{66: 'bears'},\n",
    " {66: 'ears'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'BearsEars',\n",
    "[{66: 'bears'},\n",
    " {66: 'ears'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('BerryessaSnowMountain', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'BerryessaSnowMountain',\n",
    "[{66: 'berryessa'},\n",
    " {66: 'snow'},\n",
    " {66: 'mountain'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'BerryessaSnowMountain',\n",
    "[{66: 'berryessa'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'BerryessaSnowMountain',\n",
    "[{66: 'snow'},\n",
    " {66: 'mountain'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('CanyonsoftheAncients', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'CanyonsoftheAncients',\n",
    "[{66: 'canyons'},\n",
    " {66: 'of', 'OP': '?'},\n",
    " {66: 'the', 'OP': '?'},\n",
    " {66: 'ancients'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'CanyonsoftheAncients',\n",
    "[{66: 'canyon'},\n",
    " {66: 'of', 'OP': '?'},\n",
    " {66: 'the', 'OP': '?'},\n",
    " {66: 'ancients'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_entity('CarrizoPlain', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'CarrizoPlain',\n",
    "[{66: 'carrizo'},\n",
    " {66: 'plain'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('CascadeSiskiyou', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'CascadeSiskiyou',\n",
    "[{66: 'cascade', 'OP': '?'},\n",
    " {66: 'siskiyou'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('CratersoftheMoon', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'CratersoftheMoon',\n",
    "[{66: 'craters'},\n",
    " {66: 'of'},\n",
    " {66: 'the'},\n",
    " {66: 'moon'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'CratersoftheMoon',\n",
    "[{66: 'craters', 'OP': '?'},\n",
    " {66: 'of', 'OP': '?'},\n",
    " {66: 'the', 'OP': '?'},\n",
    " {66: 'moon'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "\n",
    "nlp.matcher.add_entity('GiantSequoia', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'GiantSequoia',\n",
    "[{66: 'giant', 'OP': '?'},\n",
    " {66: 'sequoia'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('GoldButte', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'GoldButte',\n",
    "[{66: 'gold'},\n",
    " {66: 'butte'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('GrandCanyonParashant', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandCanyonParashant',\n",
    "[{66: 'grand', 'OP': '?'},\n",
    " {66: 'canyon', 'OP': '?'},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    " {IS_PUNCT: True, 'OP': '*'},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    " {66: 'parashant',}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('GrandStaircaseEscalante', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandStaircaseEscalante',\n",
    "[{66: 'grand', 'OP': '?'},\n",
    " {66: 'staircase', 'OP': '?'},\n",
    " {IS_SPACE: True, 'OP': '*'},\n",
    " {IS_PUNCT: True},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    " {66: 'escalante'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandStaircaseEscalante',\n",
    "[{66: 'grand'},\n",
    " {66: 'staircase'},\n",
    " {66: 'escalante', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandStaircaseEscalante',\n",
    "[{66: 'escalante'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'GrandStaircaseEscalante',\n",
    "[{66: 'staircase'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('HanfordReach', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'HanfordReach',\n",
    "[{66: 'hanford'},\n",
    " {66: 'reach'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('IronwoodForest', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'IronwoodForest',\n",
    "[{66: 'ironwood'},\n",
    " {66: 'forest'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('MojaveTrails', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'MojaveTrails',\n",
    "[{66: 'mojave'},\n",
    " {66: 'trails'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('OrganMountainsDesertPeaks', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'OrganMountainsDesertPeaks',\n",
    "[{66: 'organ', 'OP': '?'},\n",
    " {66: 'mountains', 'OP': '?'},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    "  {IS_PUNCT: True},\n",
    "  {IS_SPACE: True, 'OP': '*'},\n",
    " {66: 'desert', 'OP': '?'},\n",
    " {66: 'peaks'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'OrganMountainsDesertPeaks',\n",
    "[{66: 'organ'},\n",
    " {66: 'mountains', 'OP': '?'},\n",
    " {66: 'desert', 'OP': '?'},\n",
    " {66: 'peaks', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('RioGrandedelNorte', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'RioGrandedelNorte',\n",
    "[{66: 'rio', 'OP': '?'},\n",
    " {66: 'grande', 'OP': '?'},\n",
    " {66: 'del', 'OP': '?'},\n",
    " {66: 'norte'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'RioGrandedelNorte',\n",
    "[{66: 'rio'},\n",
    " {66: 'grande'},\n",
    " {66: 'del', 'OP': '?'},\n",
    " {66: 'norte', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('SandtoSnow', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'SandtoSnow',\n",
    "[{66: 'sand', 'OP': '?'},\n",
    " {66: 'to', 'OP': '?'},\n",
    " {66: 'snow'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('SanGabrielMountains', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'SanGabrielMountains',\n",
    "[{66: 'san', 'OP': '?'},\n",
    " {66: 'gabriel'},\n",
    " {66: 'mountains', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('SonoranDesert', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'SonoranDesert',\n",
    "[{66: 'sonoran'},\n",
    " {66: 'desert', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('UpperMissouriRiverBreaks', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'UpperMissouriRiverBreaks',\n",
    "[{66: 'upper', 'OP': '?'},\n",
    " {66: 'missouri'},\n",
    " {66: 'river', 'OP': '?'},\n",
    " {66: 'breaks', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('VermilionCliffs', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'VermilionCliffs',\n",
    "[{66: 'vermilion'},\n",
    " {66: 'cliffs'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('KatahdinWoodsandWaters', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'KatahdinWoodsandWaters',\n",
    "[{66: 'katahdin'},\n",
    " {66: 'woods', 'OP': '?'},\n",
    " {66: 'and', 'OP': '?'},\n",
    " {66: 'waters', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'KatahdinWoodsandWaters',\n",
    "[{66: 'katahdin'},\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('MarianasTrenchMarine', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'MarianasTrenchMarine',\n",
    "[{66: 'marianas'},\n",
    " {66: 'trench', 'OP': '?'},\n",
    " {66: 'marine', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('Seamounts', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'Seamounts',\n",
    "[{66: 'northeast'},\n",
    " {66: 'canyons'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.matcher.add_pattern(\n",
    "'Seamounts',\n",
    "[{66: 'seamounts'},\n",
    "  {66: 'marine', 'OP': '?'}\n",
    " {66: 'national', 'OP': '?'},\n",
    " {66: 'monument', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('PacificRemoteIslandsMarine', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'PacificRemoteIslandsMarine',\n",
    "[{66: 'pacific'},\n",
    " {66: 'remote', 'OP': '?'},\n",
    " {66: 'islands'},\n",
    " {66: 'marine', 'OP': '?'},\n",
    "{66: 'national', 'OP': '?'},\n",
    "{66: 'monument', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('PapahanaumokuakeaMarine', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'PapahanaumokuakeaMarine',\n",
    "[{66: 'papahanaumokuakea'},\n",
    " {66: 'marine', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "\n",
    "nlp.matcher.add_entity('RoseAtollMarine', on_match=merge_phrase)\n",
    "nlp.matcher.add_pattern(\n",
    "'RoseAtollMarine',\n",
    "[{66: 'rose'},\n",
    " {66: 'atoll', 'OP': '?'},\n",
    " {66: 'marine', 'OP': '?'}],\n",
    "label='GPE'\n",
    ")\n",
    "nlp.pipeline = [nlp.tagger, nlp.entity, nlp.matcher, nlp.parser]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some testing\n",
    "The cell below is just used for testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mons = \"\"\"Basin and Range National Monument\n",
    "Bears Ears National Monument\n",
    "Berryessa Snow Mountain\n",
    "Canyons of the Ancients\n",
    "Carrizo Plain National Monument\n",
    "Cascade Siskiyou\n",
    "Craters of the Moon National Monument\n",
    "craters of the moon monument\n",
    "Giant Sequoia\n",
    "Gold Butte\n",
    "Grand Canyon-Parashant\n",
    "Grand Canyon / Parashant National Monument\n",
    "Grand Staircase-Escalante\n",
    "Hanford Reach\n",
    "Ironwood Forest\n",
    "Mojave Trails\n",
    "Organ Mountains-Desert Peaks\n",
    "Rio Grande del Norte\n",
    "Sand to Snow\n",
    "San Gabriel Mountains\n",
    "Sonoran Desert\n",
    "Upper Missouri River Breaks\n",
    "Vermilion Cliffs\n",
    "Katahdin Woods and Waters California\n",
    "Marianas Trench Marine\n",
    "Pacific Remote Islands Marine National Monument\n",
    "Papahanaumokuakea Marine\n",
    "Rose Atoll Marine\"\"\"\n",
    "for mon in mons.split('\\n'):\n",
    "#     entity_name = ((mon.replace(' ', '')).replace('-', '')).replace(r'\\n', '')\n",
    "#     print(\"matcher.add_entity('%s', on_match=merge_phrases)\" % entity_name)\n",
    "#     pattern = [{LOWER: word} for word in mon.split()]\n",
    "#     pattern.append({LOWER: 'national', 'OP': '?'})\n",
    "#     pattern.append({LOWER: 'monument', 'OP': '?'})\n",
    "#     print('matcher.add_pattern(')\n",
    "#     print(\"'%s',\" % entity_name)\n",
    "#     pprint(pattern)\n",
    "#     print(\",label='GPE'\\n)\\n\")\n",
    "    doc = nlp('blah blah ' + mon + ' blah')\n",
    "    #matches = matcher(doc)\n",
    "#    print([w.text for w in doc])\n",
    "    print([ent.orth_ for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the slow, more accurate way to build a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(25000 docs; 4472509 tokens)\n"
     ]
    }
   ],
   "source": [
    "# this is pretty inefficient--it would be better to use \n",
    "# SpaCy's Pipe function, but I kept running into vocab issues\n",
    "\n",
    "# Only run this to update--takes 30 - 45 mins. Otherwise, just read in the corpus we've saved\n",
    "\n",
    "if 1 == 1:\n",
    "# Split recordsâ€™ content (text) field from associated metadata fields, \n",
    "# but keep them paired together for convenient loading into a textacy.Corpus\n",
    "    #docs = [textacy.doc.Doc(doc) for doc in nlp.pipe(text_stream, batch_size=1000, n_threads=4)]\n",
    "    #docs = [nlp(doc) for doc in text_stream]\n",
    "    \n",
    "    #corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream, itemwise = False)\n",
    "    corpus = textacy.Corpus(nlp)\n",
    "    #corpus.spacy_vocab = nlp.vocab\n",
    "    for comment in comments:\n",
    "        corpus.add_doc(nlp(comment['comment']), metadata=comment )\n",
    "    #corpus.save('data', name='textacy-corpus', compression='gzip')\n",
    "else:\n",
    "# or, read in the corpus we've already parsed\n",
    "    corpus = textacy.Corpus.load('data', name='textacy-corpus', compression='gzip')\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the faster, less accurate way to build a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast, but without monument matching\n",
    "text_stream, metadata_stream = textacy.fileio.split_record_fields(comments, 'comment')\n",
    "corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cool stuff you can do with textacy\n",
    "\n",
    "# counts =\n",
    "corpus.word_freqs(normalize=True, weighting='count')\n",
    "# idf = \n",
    "corpus.word_doc_freqs(normalize=True, weighting='idf')\n",
    "list(textacy.extract.ngrams(doc, 3, filter_stops=True, filter_punct=True, filter_nums=False))[:15]\n",
    "\n",
    "doc = corpus[0]\n",
    "ts = textacy.text_stats.TextStats(doc)\n",
    "print(ts.n_unique_words)\n",
    "print(ts.basic_counts)\n",
    "print(ts.readability_stats)\n",
    "\n",
    "doc.to_bag_of_terms(named_entities=True, normalize='lemma', filter_stops=True, filter_punct=True)\n",
    "\n",
    "doc.metadata\n",
    "\n",
    "corpus.word_doc_freqs(lemmatize=True, lowercase=True, weighting='count', as_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('', '', ''), 14735),\n",
       " (('Bears Ears', 'bears', 'GPE'), 13727),\n",
       " (('American', 'american', 'NORP'), 10590),\n",
       " (('Ryan Zinke', 'ryan zinke', 'PERSON'), 8247),\n",
       " (('Bears Ears National Monument', 'bears', 'GPE'), 7172),\n",
       " (('Utah', 'utah', 'GPE'), 7128),\n",
       " (('Antiquities Act', 'antiquities act', ''), 5308),\n",
       " (('America', 'america', 'GPE'), 4688),\n",
       " (('Interior', 'interior', 'ORG'), 4351),\n",
       " (('Bears Ears Inter-Tribal Coalition',\n",
       "   'bears inter - tribal coalition',\n",
       "   'GPE'),\n",
       "  4239)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a count of named entities\n",
    "\n",
    "# Extract the named entities from each comment\n",
    "from collections import Counter\n",
    "c = Counter()\n",
    "for doc in corpus:\n",
    "    ents = textacy.extract.named_entities(doc, include_types=('PERSON', 'LAW', 'ORG', 'NORP', 'GPE', 'WORK_OF_ART'), drop_determiners=False)\n",
    "    for ent in ents:\n",
    "        e = (ent.orth_, ent.lemma_, ent.label_)\n",
    "        c[e] += 1\n",
    "\n",
    "c.most_common(10) \n",
    "# should I drop determiners? Probably not.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Id': 1496,\n",
       "  'frequency': 3,\n",
       "  'lemma': 'daly city',\n",
       "  'pos': 'GPE',\n",
       "  'value': 'Daly City'},\n",
       " {'Id': 1497,\n",
       "  'frequency': 3,\n",
       "  'lemma': 'kwwnm',\n",
       "  'pos': 'ORG',\n",
       "  'value': 'KWWNM'},\n",
       " {'Id': 1498,\n",
       "  'frequency': 3,\n",
       "  'lemma': 'congressional',\n",
       "  'pos': 'NORP',\n",
       "  'value': 'Congressional'},\n",
       " {'Id': 1499,\n",
       "  'frequency': 3,\n",
       "  'lemma': 'ashland , or',\n",
       "  'pos': 'ORG',\n",
       "  'value': 'Ashland, OR'},\n",
       " {'Id': 1500,\n",
       "  'frequency': 3,\n",
       "  'lemma': 'alamo',\n",
       "  'pos': 'ORG',\n",
       "  'value': 'Alamo'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents = list()\n",
    "for ent in c.most_common(1500):\n",
    "    ents.append({'value': ent[0][0],\n",
    "                'lemma': ent[0][1],\n",
    "                'pos': ent[0][2],\n",
    "                'frequency': ent[1]})\n",
    "\n",
    "for i, row in enumerate(ents):\n",
    "    ents[i][\"Id\"] = i+1\n",
    "\n",
    "entities = set([ent['value'] for ent in ents])\n",
    "ents[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'\"Review of Certain National Monuments Established Since 1996'},\n",
      " {'Grand Staircase-Escalante Canyon National Monuments'},\n",
      " {'Upper Missouri River Breaks National Monument',\n",
      "  'Upper Missouri River Breaks National Monument in'},\n",
      " {'Review of Certain National Monuments Established'},\n",
      " {'Pacific Remote Islands Marine National Monument'},\n",
      " {'Organ Mountains Desert Peaks National Monument',\n",
      "  'Organ Mountains-Desert Peaks National Monument'},\n",
      " {'Grand Staircase - Escalante National Monument',\n",
      "  'Grand Staircase-Escalante National Monuments'},\n",
      " {'Grand Staircase Escalante National',\n",
      "  'Grand Staircase Escalante National Monuments',\n",
      "  'Grand Staircase-Escalante National Monument'},\n",
      " {'Grand Staircase Escalante National Monument',\n",
      "  'Grand Staircase/Escalante National Monument'},\n",
      " {'Papah&#257;naumoku&#257;kea Marine Monument'},\n",
      " {'Katahdin Woods & Waters National Monument',\n",
      "  'Katahdin Woods and Waters National',\n",
      "  'Katahdin Woods and Waters National Monument'},\n",
      " {'Canyon of the Ancients National Monument',\n",
      "  'Canyons of the Ancients National Monument'},\n",
      " {'Papahnaumokukea Marine National Monument', 'Marine National Monument'},\n",
      " {'National Wildlife Federation Action Fund'},\n",
      " {'Grand Canyon-Parashant National Monument'},\n",
      " {'San Gabriel Mountains National', 'San Gabriel Mountains National Monument'},\n",
      " {'Rio Grande del Norte National Monument'},\n",
      " {'Federal Land Policy and Management Act'},\n",
      " {'National Congress of American Indians'},\n",
      " {'Craters of the Moon National Monument'}]\n"
     ]
    }
   ],
   "source": [
    "# group things together, just to make the lookup table building a little easier\n",
    "textacy_variants = textacy.keyterms.aggregate_term_variants(entities, fuzzy_dedupe=True)\n",
    "import pprint\n",
    "pprint.pprint(textacy_variants[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[''],\n",
       " ['\"'],\n",
       " ['\"Review of Certain National Monuments Established Since 1996'],\n",
       " ['\"Science Monument\"yielding'],\n",
       " [', Escalante River', ', Escalante'],\n",
       " [\"- Protection of America's\", \"Protection of America's\"],\n",
       " ['--Susanne Hilty'],\n",
       " ['. Escalante'],\n",
       " ['1,243-mile Columbia River'],\n",
       " ['3'],\n",
       " ['ACT'],\n",
       " ['ALL Americans', 'ALL AMERICANS'],\n",
       " ['ALL National Monuments'],\n",
       " ['AMA'],\n",
       " ['AMERICA'],\n",
       " ['AMERICAN'],\n",
       " ['AMERICAN SAMOA'],\n",
       " ['ANTIQUITIES'],\n",
       " ['ANYONE'],\n",
       " ['ASK']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the varients. But, don't actually do this!\n",
    "# varients = [list(var) for var in textacy_variants]\n",
    "# def getKey(item):\n",
    "#     return item[0]\n",
    "# varients = sorted(varients, key=getKey)\n",
    "# varients[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a dataset with the named entities and frequencies\n",
    "# but dont actually do this\n",
    "# with open('ents.csv', 'w') as f:\n",
    "#     writer = csv.DictWriter(f, ['Id', 'value', 'lemma', 'pos', 'frequency'])\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataset used to create the named entity resolution stuff\n",
    "# this is the part where I painstakenly go through the resulting\n",
    "# spreadsheet and manually map instances to their canonical value\n",
    "with open('ents.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # flatten the grouped entities\n",
    "    # to do: include a count of each\n",
    "    for item in [item for sublist in textacy_variants for item in sublist]:\n",
    "        writer.writerow([item])\n",
    "# save the resulting file as data/entity-resolution.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag docs with named entities\n",
    "1. read in 'data/entity-resolution.csv'\n",
    "2. tag each document with the named entities it contains\n",
    "3. export as to csv (data/named-entities.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total named instances 1132\n",
      "unique named entities 561\n"
     ]
    }
   ],
   "source": [
    "# read entities into a list of tuples of form (instance, entity)\n",
    "with open('data/entity-resolution.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    er = [ent for ent in reader]\n",
    "er = [(item['instance'], item['canonical']) for item in er]\n",
    "lookup = {e[0]: e[1] for e in er}\n",
    "\n",
    "entities = set(lookup.values())\n",
    "lookup.update({e: e for e in entities})\n",
    "instances = set(lookup.keys()).union(entities)\n",
    "\n",
    "print('total named instances', str(len(instances)))\n",
    "print('unique named entities', str(len(entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 561)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty dataframe with\n",
    "# rows: document_ids\n",
    "# columns: canonical named entities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(False, index=[doc.metadata.get('document_id') for doc in corpus], columns=entities, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each doc, get list of named entities, and update df\n",
    "# setting the appropriate canonical name columns to true for matches\n",
    "from spacy.parts_of_speech import DET # to remove determiners\n",
    "for doc in corpus:\n",
    "    row_idx = doc.metadata.get('document_id')\n",
    "    nes = [ne if ne[0].pos != DET else ne[1:] for ne in doc.spacy_doc.ents]\n",
    "    nes = set([ne.orth_ for ne in nes])\n",
    "    df.loc[row_idx, [lookup[v] for v in instances.intersection(nes)]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert bool to 0/1 and write to file\n",
    "(df*1).to_csv('data/named-entities.csv', index_label='document_id',) # mult. by 1 to convert boolean to 0/1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
