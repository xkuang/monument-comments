{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook:\n",
    "- splits labelled data into train, validation, and test sets\n",
    "- extract features and evaluation performance using 10-fold cross validation\n",
    "- optimize select model with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# comments labelled: 2500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>DOI-2017-0002-138105</td>\n",
       "      <td>I strongly urge you to continue to protect all...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 document_id  \\\n",
       "count                   2500   \n",
       "unique                  2500   \n",
       "top     DOI-2017-0002-138105   \n",
       "freq                       1   \n",
       "\n",
       "                                                  comment sentiment  \n",
       "count                                                2500      2500  \n",
       "unique                                               2500         3  \n",
       "top     I strongly urge you to continue to protect all...       pos  \n",
       "freq                                                    1      2363  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the data from CSV\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "labelledComments = pd.read_csv(\"data/comments-to-label.csv\", usecols=['document_id', 'comment', 'sentiment']).dropna()\n",
    "labelledComments['comment'] = labelledComments['comment'].map(lambda x: re.sub(\"\\r\", \" \", x))\n",
    "\n",
    "print(\"# comments labelled:\", labelledComments.shape[0])\n",
    "labelledComments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos    2363\n",
      "neg     120\n",
      "neu      17\n",
      "Name: sentiment, dtype: int64\n",
      "Index for class labels: Index(['neg', 'neu', 'pos'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# View class distribution\n",
    "\n",
    "labelledComments['sentiment'] = labelledComments['sentiment'].astype('category')\n",
    "print(labelledComments.sentiment.value_counts())\n",
    "print(\"Index for class labels:\", labelledComments.sentiment.cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train: 1750\n",
      "# validate: 500\n",
      "# test: 250\n"
     ]
    }
   ],
   "source": [
    "# Split labelled dataset into train (.7), validation (.2), and test (.1) sets \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train, validate, test = np.split(labelledComments.sample(frac=1), [int(.7*len(labelledComments)), int(.9*len(labelledComments))])\n",
    "\n",
    "train.to_csv('data/comments-train.csv', index=False)\n",
    "validate.to_csv('data/comments-validate.csv', index=False)\n",
    "test.to_csv('data/comments-test.csv', index=False)\n",
    "\n",
    "print(\"# train:\", train.shape[0])\n",
    "print(\"# validate:\", validate.shape[0])\n",
    "print(\"# test:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Load data from file\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# train = pd.read_csv(\"data/comments-train.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "# validate = pd.read_csv(\"data/comments-validate.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "# test = pd.read_csv(\"data/comments-test.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "# print(\"# train:\", train.shape[0])\n",
    "# print(\"# validate:\", validate.shape[0])\n",
    "# print(\"# test:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "- baseline\n",
    "- content v.s. style test: stopwords removal, occurrence count, and tf-idf\n",
    "- token-level analysis: stemming and lemmatization\n",
    "- full text v.s. select sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a bag-of-word matrix \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# baseline: unigram + bigram (binary)\n",
    "unigram_bigram_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram = unigram_bigram_v.fit_transform(train[\"comment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords removal, occurrence count, and tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simply change the parameters of CountVectorizer and TfidfVectorizer \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# unigram + bigram + stopwords removal\n",
    "unigram_bigram_noStopword_v = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_noStopword = unigram_bigram_noStopword_v.fit_transform(train[\"comment\"])\n",
    "\n",
    "# unigram + bigram (count)\n",
    "unigram_bigram_count_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "unigram_bigram_count = unigram_bigram_count_v.fit_transform(train[\"comment\"])\n",
    "\n",
    "# unigram + bigram (tf-idf)\n",
    "unigram_bigram_tfidf_v = TfidfVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "unigram_bigram_tfidf = unigram_bigram_tfidf_v.fit_transform(train[\"comment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Customize tokenizers\n",
    "\n",
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "class PorterStemmerTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = gensim.utils.simple_preprocess(doc, deacc=True, min_len=2)\n",
    "        return [self.stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = gensim.utils.simple_preprocess(doc, deacc=True, min_len=2)\n",
    "        return [self.wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pass customized tokenizer objects to CountVectorizer\n",
    "\n",
    "# unigram + bigram + stemming\n",
    "unigram_bigram_stem_v = CountVectorizer(tokenizer=PorterStemmerTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_stem = unigram_bigram_stem_v.fit_transform(train[\"comment\"])\n",
    "\n",
    "# unigram + bigram + lemmatization\n",
    "unigram_bigram_lemma_v = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_lemma = unigram_bigram_lemma_v.fit_transform(train[\"comment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select sentences\n",
    "- split text into sentences\n",
    "- remove the signature line if there is any\n",
    "- extract the first three and last three sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Remove default letter opening, i.e. \"Dear Secretary Ryan Zinke, \"\n",
    "\n",
    "# opening = \"(Dear Secretary Ryan Zinke,|Dear Secretary Zinke,)\"\n",
    "# labelledComments['comment'] = labelledComments['comment'].map(lambda x: re.sub(opening, \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write functions to split sentences, remove signature and extract sentences\n",
    "\n",
    "import re\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    '''\n",
    "    Function to break text (astring) into sentences (a list of strings).\n",
    "    # Ref: https://stackoverflow.com/a/31505798\n",
    "    '''\n",
    "    caps = \"([A-Z])\"\n",
    "    digits = \"([0-9])\"\n",
    "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "    starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip())>1]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def remove_signature(sentences):\n",
    "    '''\n",
    "    Function to remove the signature line from comment sentences\n",
    "    '''\n",
    "    closing = ['thank you','thanks','sincerely','regards']\n",
    "    last_sent = sentences[-1].lower()\n",
    "    if any(term in last_sent for term in closing):\n",
    "        return sentences[:-1]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def select_sentences(text, first_n, last_m):\n",
    "    '''\n",
    "    Function to extract the first N and the last M sentences from text, excluding the signature line.\n",
    "    '''\n",
    "    sentences = split_into_sentences(text)\n",
    "    sentences = remove_signature(sentences)\n",
    "    return \" \".join(sentences[:first_n]+sentences[-last_m:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract first and last 3 sentences from text and transform them to vectors\n",
    "\n",
    "train['first3last3'] = train['comment'].map(lambda x: select_sentences(x,3,3))\n",
    "\n",
    "# unigram + bigram (first and last 3 sentences)\n",
    "unigram_bigram_sents_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_sents = unigram_bigram_sents_v.fit_transform(train[\"first3last3\"])\n",
    "\n",
    "# unigram + bigram + stemming (first and last 3 sentences)\n",
    "unigram_bigram_sents_stem_v = CountVectorizer(tokenizer=PorterStemmerTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_sents_stem = unigram_bigram_sents_stem_v.fit_transform(train[\"first3last3\"])\n",
    "\n",
    "# unigram + bigram + lemmatization (first and last 3 sentences)\n",
    "unigram_bigram_sents_lemma_v = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_sents_lemma = unigram_bigram_sents_lemma_v.fit_transform(train[\"first3last3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate models with 10-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics.classification import cohen_kappa_score\n",
    "\n",
    "\n",
    "def run_CV(feature_matrices, labels, model_list, num_folds):\n",
    "    '''\n",
    "    Build a classifier with for each feature matrix and model, and evaluate with \n",
    "    a N-fold cross validation. The performance is measured with kappa statistic.\n",
    "    The output is a list of performance results per model. \n",
    "    '''\n",
    "    cv = KFold(num_folds)    \n",
    "    result = []\n",
    "    for model in model_list:\n",
    "        model_scores = []\n",
    "        for X in feature_matrices:\n",
    "            kappa_scores = []\n",
    "            for train_index, test_index in cv.split(X): # cv\n",
    "                model.fit(X[train_index], labels.iloc[train_index]) # fit a logit model to the data\n",
    "                ypred = model.predict(X[test_index]) # make predictions\n",
    "                kappa = cohen_kappa_score(labels.iloc[test_index], ypred) # get kappa score\n",
    "                kappa_scores.append(kappa)\n",
    "            model_scores.append(np.mean(kappa_scores))\n",
    "        result.append(model_scores)\n",
    "    return result\n",
    "\n",
    "\n",
    "logit = LogisticRegression()\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=400, random_state=1)\n",
    "labels = train['sentiment'].cat.codes\n",
    "\n",
    "matrices = [unigram_bigram, \n",
    "            unigram_bigram_noStopword, unigram_bigram_count, unigram_bigram_tfidf, \n",
    "            unigram_bigram_stem, unigram_bigram_lemma,\n",
    "            unigram_bigram_sents, unigram_bigram_sents_stem, unigram_bigram_sents_lemma]\n",
    "\n",
    "result = run_CV(matrices, labels, [logit,svm], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logit</th>\n",
       "      <th>svm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unigram_bigram</th>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.638145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_bigram_noStopword</th>\n",
       "      <td>0.608708</td>\n",
       "      <td>0.664825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_bigram_count</th>\n",
       "      <td>0.630836</td>\n",
       "      <td>0.617871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_bigram_tfidf</th>\n",
       "      <td>0.235604</td>\n",
       "      <td>0.349349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_bigram_stem</th>\n",
       "      <td>0.685998</td>\n",
       "      <td>0.677692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_bigram_lemma</th>\n",
       "      <td>0.640121</td>\n",
       "      <td>0.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_bigram_sents</th>\n",
       "      <td>0.601051</td>\n",
       "      <td>0.639710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_bigram_sents_stem</th>\n",
       "      <td>0.632646</td>\n",
       "      <td>0.673983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_bigram_sents_lemma</th>\n",
       "      <td>0.621810</td>\n",
       "      <td>0.659006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               logit       svm\n",
       "unigram_bigram              0.635220  0.638145\n",
       "unigram_bigram_noStopword   0.608708  0.664825\n",
       "unigram_bigram_count        0.630836  0.617871\n",
       "unigram_bigram_tfidf        0.235604  0.349349\n",
       "unigram_bigram_stem         0.685998  0.677692\n",
       "unigram_bigram_lemma        0.640121  0.672100\n",
       "unigram_bigram_sents        0.601051  0.639710\n",
       "unigram_bigram_sents_stem   0.632646  0.673983\n",
       "unigram_bigram_sents_lemma  0.621810  0.659006"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display results in dataframe\n",
    "\n",
    "df = pd.DataFrame(result).transpose()\n",
    "df.columns = ['logit', 'svm']\n",
    "df.index = ['unigram_bigram', \n",
    "            'unigram_bigram_noStopword', 'unigram_bigram_count', 'unigram_bigram_tfidf', \n",
    "            'unigram_bigram_stem', 'unigram_bigram_lemma',\n",
    "            'unigram_bigram_sents', 'unigram_bigram_sents_stem', 'unigram_bigram_sents_lemma']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg: mani nation, depart, first time, neighbor commun, but in, and my, it design, human be, cannot allow, list\n",
      "neu: fit, explor and, re creat, joy, fate, extinct, co, over to, protect wild, by keep\n",
      "pos: grand, feder, for peopl, profit for, indigen peopl, econom boost, it citizen, know the, proud of, preserv more\n"
     ]
    }
   ],
   "source": [
    "# Choose one model to examine the most predictive features per class\n",
    "\n",
    "def print_topN_features(vectorizer, model, class_labels, n):\n",
    "    \"\"\"\n",
    "    Print features with the highest coefficient values, per class\n",
    "    \"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        topN = np.argsort(model.coef_[i])[-n:]\n",
    "        print(\"%s: %s\" % (class_label,\", \".join(feature_names[j] for j in topN)))\n",
    "\n",
    "print_topN_features(unigram_bigram_stem_v, svm, ['neg', 'neu', 'pos'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimize\n",
    "- introduce additional features\n",
    "- adjust class weights\n",
    "- tune parameter using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
