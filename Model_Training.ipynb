{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook:\n",
    "- splits labelled data into train, validation, and test sets\n",
    "- extract features and evaluation performance using 10-fold cross validation\n",
    "- optimize select model with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    '''\n",
    "    Function to break text (astring) into sentences (a list of strings).\n",
    "    # Ref: https://stackoverflow.com/a/31505798\n",
    "    '''\n",
    "    caps = \"([A-Z])\"\n",
    "    digits = \"([0-9])\"\n",
    "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "    starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip())>1]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def remove_signature(sentences):\n",
    "    '''\n",
    "    Function to remove the signature line from comment sentences\n",
    "    '''\n",
    "    closing = ['thank you','thanks','sincerely','regards']\n",
    "    last_sent = sentences[-1].lower()\n",
    "    if any(term in last_sent for term in closing):\n",
    "        return sentences[:-1]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def select_sentences(text, first_n, last_m):\n",
    "    '''\n",
    "    Function to extract the first N and the last M sentences from text, excluding the signature line.\n",
    "    '''\n",
    "    sentences = split_into_sentences(text)\n",
    "    sentences = remove_signature(sentences)\n",
    "    return \" \".join(sentences[:first_n]+sentences[-last_m:])\n",
    "\n",
    "\n",
    "def hasContent(sentences):\n",
    "    if len(sentences) == 0:\n",
    "        return False\n",
    "    if sentences[0].find('Leave your personal comment here') != -1 \\\n",
    "    or (len(sentences) == 2 and len(sentences[0]) < 30 and \\\n",
    "        sentences[0].find('Dear Secretary Ryan Zinke,') != -1):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def tokenize(text, minLength=3):\n",
    "    return gensim.utils.simple_preprocess(text, deacc=True, min_len=minLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Customize tokenizers for feature extraction\n",
    "\n",
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "class PorterStemmerTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = gensim.utils.simple_preprocess(doc, deacc=True, min_len=2)\n",
    "        return [self.stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = gensim.utils.simple_preprocess(doc, deacc=True, min_len=2)\n",
    "        return [self.wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics.classification import cohen_kappa_score\n",
    "\n",
    "\n",
    "def run_CV(feature_matrices, labels, model_list, num_folds):\n",
    "    '''\n",
    "    Build a classifier with for each feature matrix and model, and evaluate with \n",
    "    a N-fold cross validation. The performance is measured with kappa statistic.\n",
    "    The output is a list of performance results per model. \n",
    "    '''\n",
    "    cv = KFold(num_folds)    \n",
    "    result = []\n",
    "    for model in model_list:\n",
    "        model_scores = []\n",
    "        for X in feature_matrices:\n",
    "            kappa_scores = []\n",
    "            for train_index, test_index in cv.split(X): # cv\n",
    "                model.fit(X[train_index], labels.iloc[train_index]) # fit a logit model to the data\n",
    "                ypred = model.predict(X[test_index]) # make predictions\n",
    "                kappa = cohen_kappa_score(labels.iloc[test_index], ypred) # get kappa score\n",
    "                kappa_scores.append(kappa)\n",
    "            model_scores.append(np.mean(kappa_scores))\n",
    "        result.append(model_scores)\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_on_test_set(vectorizers, model, train_data, test_data):\n",
    "    scores = []\n",
    "    for X in vectorizers:\n",
    "        train_vectors = X.fit_transform(train_data['comment']) # construct feature vectors (train)\n",
    "        clf = model.fit(train_vectors, train_data['sentiment'].cat.codes) # fit model\n",
    "        test_vectors = X.transform(test_data['comment']) # construct feature vectors (test)\n",
    "        predicted = clf.predict(test_vectors) # make predictions\n",
    "        kappa = cohen_kappa_score(test_data['sentiment'].cat.codes, predicted) # get kappa score\n",
    "        scores.append(kappa)\n",
    "    print(scores)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Displaying and styling\n",
    "\n",
    "\n",
    "def print_topN_features(vectorizer, model, class_labels, n):\n",
    "    \"\"\"\n",
    "    Print features with the highest coefficient values, per class\n",
    "    \"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    features = {}\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        topN = np.argsort(model.coef_[i])[-n:]\n",
    "        features[class_label] = [feature_names[j] for j in topN]\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# comments labelled: 3388\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>DOI-2017-0002-135005</td>\n",
       "      <td>DO NOT change the national monument designatio...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 document_id  \\\n",
       "count                   3388   \n",
       "unique                  3388   \n",
       "top     DOI-2017-0002-135005   \n",
       "freq                       1   \n",
       "\n",
       "                                                  comment sentiment  \n",
       "count                                                3388      3388  \n",
       "unique                                               3388         3  \n",
       "top     DO NOT change the national monument designatio...       pos  \n",
       "freq                                                    1      2820  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the data from CSV\n",
    "\n",
    "labelledComments = pd.read_csv(\"data/comments-merged.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "print(\"# comments labelled:\", labelledComments.shape[0])\n",
    "labelledComments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos    2820\n",
      "neg     541\n",
      "neu      27\n",
      "Name: sentiment, dtype: int64\n",
      "Index for class labels: Index(['neg', 'neu', 'pos'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# View class distribution\n",
    "\n",
    "labelledComments['sentiment'] = labelledComments['sentiment'].astype('category')\n",
    "print(labelledComments.sentiment.value_counts())\n",
    "print(\"Index for class labels:\", labelledComments.sentiment.cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train: 2710\n",
      "# test: 1017\n"
     ]
    }
   ],
   "source": [
    "# Split labelled dataset into train (.8) and test (.2) sets \n",
    "\n",
    "train, test = np.split(labelledComments.sample(frac=1), [int(.8*len(labelledComments))])\n",
    "\n",
    "train.to_csv('data/comments-train.csv', index=False)\n",
    "test.to_csv('data/comments-test.csv', index=False)\n",
    "\n",
    "print(\"# train:\", train.shape[0])\n",
    "print(\"# test:\", validate.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Load data from file\n",
    "\n",
    "# train = pd.read_csv(\"data/comments-train.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "# validate = pd.read_csv(\"data/comments-validate.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "# test = pd.read_csv(\"data/comments-test.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "# print(\"# train:\", train.shape[0])\n",
    "# print(\"# validate:\", validate.shape[0])\n",
    "# print(\"# test:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "- baseline\n",
    "- content v.s. style test: stopwords removal, occurrence count, and tf-idf\n",
    "- token-level analysis: stemming and lemmatization\n",
    "- full text v.s. select sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a bag-of-word matrix \n",
    "\n",
    "docs = train['comment']\n",
    "\n",
    "# baseline: unigram + bigram (binary)\n",
    "unigram_bigram_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram = unigram_bigram_v.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords removal, occurrence count, and tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simply change the parameters of CountVectorizer and TfidfVectorizer \n",
    "\n",
    "# unigram + bigram + stopwords removal\n",
    "unigram_bigram_noStopword_v = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_noStopword = unigram_bigram_noStopword_v.fit_transform(docs)\n",
    "\n",
    "# unigram + bigram (count)\n",
    "unigram_bigram_count_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "unigram_bigram_count = unigram_bigram_count_v.fit_transform(docs)\n",
    "\n",
    "# unigram + bigram (tf-idf)\n",
    "unigram_bigram_tfidf_v = TfidfVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "unigram_bigram_tfidf = unigram_bigram_tfidf_v.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pass customized tokenizer objects to CountVectorizer\n",
    "\n",
    "# unigram + bigram + stemming\n",
    "unigram_bigram_stem_v = CountVectorizer(tokenizer=PorterStemmerTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_stem = unigram_bigram_stem_v.fit_transform(docs)\n",
    "\n",
    "# unigram + bigram + lemmatization\n",
    "unigram_bigram_lemma_v = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_lemma = unigram_bigram_lemma_v.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select sentences\n",
    "- split text into sentences\n",
    "- remove the signature line if there is any\n",
    "- extract the first three and last three sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Remove default letter opening, i.e. \"Dear Secretary Ryan Zinke, \"\n",
    "\n",
    "# opening = \"(Dear Secretary Ryan Zinke,|Dear Secretary Zinke,)\"\n",
    "# labelledComments['comment'] = labelledComments['comment'].map(lambda x: re.sub(opening, \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract first and last 3 sentences from text and transform them to vectors\n",
    "\n",
    "train['first3last3'] = train['comment'].map(lambda x: select_sentences(x,3,3))\n",
    "sents = train['first3last3'] \n",
    "\n",
    "# unigram + bigram (first and last 3 sentences)\n",
    "unigram_bigram_sents_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_sents = unigram_bigram_sents_v.fit_transform(sents)\n",
    "\n",
    "# unigram + bigram + stemming (first and last 3 sentences)\n",
    "unigram_bigram_sents_stem_v = CountVectorizer(tokenizer=PorterStemmerTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_sents_stem = unigram_bigram_sents_stem_v.fit_transform(sents)\n",
    "\n",
    "# unigram + bigram + lemmatization (first and last 3 sentences)\n",
    "unigram_bigram_sents_lemma_v = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_sents_lemma = unigram_bigram_sents_lemma_v.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate models with 3-fold cross validation\n",
    "\n",
    "logit = LogisticRegression()\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=1000, random_state=1)\n",
    "labels = train['sentiment'].cat.codes\n",
    "\n",
    "matrices = [unigram_bigram, \n",
    "            unigram_bigram_noStopword, unigram_bigram_count, unigram_bigram_tfidf, \n",
    "            unigram_bigram_stem, unigram_bigram_lemma,\n",
    "            unigram_bigram_sents, unigram_bigram_sents_stem, unigram_bigram_sents_lemma]\n",
    "\n",
    "result = run_CV(matrices, labels, [logit,svm], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style  type=\"text/css\" >\n",
       "        \n",
       "        \n",
       "            #T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow4_col0 {\n",
       "            \n",
       "                background-color:  yellow;\n",
       "            \n",
       "            }\n",
       "        \n",
       "            #T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow4_col1 {\n",
       "            \n",
       "                background-color:  yellow;\n",
       "            \n",
       "            }\n",
       "        \n",
       "        </style>\n",
       "\n",
       "        <table id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" None>\n",
       "        \n",
       "\n",
       "        <thead>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th class=\"blank\">\n",
       "                \n",
       "                <th class=\"col_heading level0 col0\">logit\n",
       "                \n",
       "                <th class=\"col_heading level0 col1\">svm\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "        </thead>\n",
       "        <tbody>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" class=\"row_heading level1 row0\">\n",
       "                    unigram_bigram\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow0_col0\" class=\"data row0 col0\">\n",
       "                    0.856036\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow0_col1\" class=\"data row0 col1\">\n",
       "                    0.858823\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" class=\"row_heading level1 row1\">\n",
       "                    unigram_bigram_noStopword\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow1_col0\" class=\"data row1 col0\">\n",
       "                    0.855881\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow1_col1\" class=\"data row1 col1\">\n",
       "                    0.839656\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" class=\"row_heading level1 row2\">\n",
       "                    unigram_bigram_count\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow2_col0\" class=\"data row2 col0\">\n",
       "                    0.854713\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow2_col1\" class=\"data row2 col1\">\n",
       "                    0.856928\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" class=\"row_heading level1 row3\">\n",
       "                    unigram_bigram_tfidf\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow3_col0\" class=\"data row3 col0\">\n",
       "                    0.719346\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow3_col1\" class=\"data row3 col1\">\n",
       "                    0.805107\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" class=\"row_heading level1 row4\">\n",
       "                    unigram_bigram_stem\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow4_col0\" class=\"data row4 col0\">\n",
       "                    0.865571\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow4_col1\" class=\"data row4 col1\">\n",
       "                    0.868662\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" class=\"row_heading level1 row5\">\n",
       "                    unigram_bigram_lemma\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow5_col0\" class=\"data row5 col0\">\n",
       "                    0.857727\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow5_col1\" class=\"data row5 col1\">\n",
       "                    0.857129\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" class=\"row_heading level1 row6\">\n",
       "                    unigram_bigram_sents\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow6_col0\" class=\"data row6 col0\">\n",
       "                    0.853581\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow6_col1\" class=\"data row6 col1\">\n",
       "                    0.853323\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" class=\"row_heading level1 row7\">\n",
       "                    unigram_bigram_sents_stem\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow7_col0\" class=\"data row7 col0\">\n",
       "                    0.859907\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow7_col1\" class=\"data row7 col1\">\n",
       "                    0.848841\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175d\" class=\"row_heading level1 row8\">\n",
       "                    unigram_bigram_sents_lemma\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow8_col0\" class=\"data row8 col0\">\n",
       "                    0.843115\n",
       "                \n",
       "                <td id=\"T_998e4a3e_5ef3_11e7_846a_f45c89b9175drow8_col1\" class=\"data row8 col1\">\n",
       "                    0.836457\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "        </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<pandas.formats.style.Styler at 0x11964aba8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display results in dataframe and highlight the maximum in each column\n",
    "\n",
    "df = pd.DataFrame(result).transpose()\n",
    "df.columns = ['logit', 'svm']\n",
    "df.index = ['unigram_bigram', \n",
    "            'unigram_bigram_noStopword', 'unigram_bigram_count', 'unigram_bigram_tfidf', \n",
    "            'unigram_bigram_stem', 'unigram_bigram_lemma',\n",
    "            'unigram_bigram_sents', 'unigram_bigram_sents_stem', 'unigram_bigram_sents_lemma']\n",
    "\n",
    "df.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>have access</td>\n",
       "      <td>enjoy bear</td>\n",
       "      <td>all fifteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>keep our</td>\n",
       "      <td>even though</td>\n",
       "      <td>marvel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plant and</td>\n",
       "      <td>forc</td>\n",
       "      <td>open for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>growth</td>\n",
       "      <td>enhanc</td>\n",
       "      <td>nation recreat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>materi</td>\n",
       "      <td>furthermor</td>\n",
       "      <td>myriad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>may never</td>\n",
       "      <td>it archaeolog</td>\n",
       "      <td>capabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>low</td>\n",
       "      <td>firmli</td>\n",
       "      <td>kind of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of conserv</td>\n",
       "      <td>countri these</td>\n",
       "      <td>comment as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>harm</td>\n",
       "      <td>can keep</td>\n",
       "      <td>like our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ad</td>\n",
       "      <td>monument boundari</td>\n",
       "      <td>is disgrac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>call the</td>\n",
       "      <td>be seen</td>\n",
       "      <td>ken burn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pleas leav</td>\n",
       "      <td>help defin</td>\n",
       "      <td>land need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>land these</td>\n",
       "      <td>help make</td>\n",
       "      <td>modifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>for us</td>\n",
       "      <td>cannot believ</td>\n",
       "      <td>honor and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>man</td>\n",
       "      <td>for etern</td>\n",
       "      <td>can provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>develop as</td>\n",
       "      <td>boy scout</td>\n",
       "      <td>from local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>know how</td>\n",
       "      <td>breathtak</td>\n",
       "      <td>like bear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mall</td>\n",
       "      <td>can onli</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in gateway</td>\n",
       "      <td>monument contain</td>\n",
       "      <td>for it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>alik substanti</td>\n",
       "      <td>renew energi</td>\n",
       "      <td>leader in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               neg                neu             pos\n",
       "0      have access         enjoy bear     all fifteen\n",
       "1         keep our        even though          marvel\n",
       "2        plant and               forc        open for\n",
       "3           growth             enhanc  nation recreat\n",
       "4           materi         furthermor          myriad\n",
       "5        may never      it archaeolog          capabl\n",
       "6              low             firmli         kind of\n",
       "7       of conserv      countri these      comment as\n",
       "8             harm           can keep        like our\n",
       "9               ad  monument boundari      is disgrac\n",
       "10        call the            be seen        ken burn\n",
       "11      pleas leav         help defin       land need\n",
       "12      land these          help make          modifi\n",
       "13          for us      cannot believ       honor and\n",
       "14             man          for etern      can provid\n",
       "15      develop as          boy scout      from local\n",
       "16        know how          breathtak       like bear\n",
       "17            mall           can onli             had\n",
       "18      in gateway   monument contain          for it\n",
       "19  alik substanti       renew energi       leader in"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose one model to examine the most predictive features per class\n",
    "\n",
    "print_topN_features(unigram_bigram_stem_v, svm, ['neg', 'neu', 'pos'], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with additional negative examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and slice data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in data\n",
    "\n",
    "labelled = pd.read_csv(\"data/comments-merged.csv\", usecols=['document_id', 'comment', 'sentiment']).dropna()\n",
    "labelled['comment'] = labelled['comment'].map(lambda x: re.sub(\"\\r\", \" \", x))\n",
    "\n",
    "additional = pd.read_csv(\"data/relabelled.csv\", usecols=['document_id', 'comment', 'Actual']).dropna()\n",
    "additional.columns = ['document_id', 'comment', 'sentiment']\n",
    "additional['comment'] = additional['comment'].map(lambda x: re.sub(\"\\r\", \" \", x))\n",
    "additional = additional.replace({'sentiment': {'Positive': 'pos', 'Neutral': 'neu', 'Negative': 'neg'}})\n",
    "\n",
    "print(\"# labelled comments:\", labelled.shape[0])\n",
    "print(\"# additional comments:\", additional.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare datasets for training and testing\n",
    "# Approach 1: use a subset of 3000 labelled data as test set\n",
    "\n",
    "# divide into training set (control group) and test set \n",
    "control_train, exp_test = np.split(labelled.sample(frac=1), [int(.7*len(labelled))])\n",
    "\n",
    "# add additional examples to the control group to form treatment group\n",
    "treatment_train = control_train.append(additional, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Approach 2: use CWP samples as test set\n",
    "\n",
    "# cwp = pd.read_csv(\"data/cwp-sample.csv\", usecols=['ID', 'comment', 'Sentiment']).dropna()\n",
    "# cwp.columns = ['document_id', 'comment', 'sentiment']\n",
    "# cwp['comment'] = cwp['comment'].map(lambda x: re.sub(\"\\r\", \" \", x))\n",
    "# cwp = cwp.replace({'sentiment': {'Positive': 'pos', 'Neutral': 'neu', 'Negative': 'neg'}})\n",
    "\n",
    "# cwp['sentences'] = cwp['comment'].map(lambda x: split_into_sentences(x))\n",
    "# cwp = cwp[cwp['sentences'].map(hasContent)]\n",
    "# cwp['first_two_sents'] = cwp['sentences'].map(lambda x: \" \".join([\" \".join(tokenize(sent,2)) for sent in x[:2]]))\n",
    "# exp_test = cwp.drop_duplicates(subset=['first_two_sents'])[['document_id', 'comment', 'sentiment']]\n",
    "\n",
    "\n",
    "# # use current labelled data as train set for the control group\n",
    "# control_train = labelled\n",
    "\n",
    "# # add additional examples to the control group to form treatment group\n",
    "# treatment_train = control_train.append(additional, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop duplicates \n",
    "\n",
    "# based on document id \n",
    "treatment_train = treatment_train.drop_duplicates(subset=['document_id'])\n",
    "\n",
    "# based on first two sentences of comment texts\n",
    "treatment_train['sentences'] = treatment_train['comment'].map(lambda x: split_into_sentences(x))\n",
    "treatment_train = treatment_train[treatment_train['sentences'].map(hasContent)]\n",
    "treatment_train['first_two_sents'] = treatment_train['sentences'].map(lambda x: \" \".join([\" \".join(tokenize(sent,2)) for sent in x[:2]]))\n",
    "treatment_train = treatment_train.drop_duplicates(subset=['first_two_sents'])\n",
    "treatment_train = treatment_train[['document_id', 'comment', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the data type of sentiment column\n",
    "\n",
    "control_train['sentiment'] = control_train['sentiment'].astype('category')\n",
    "treatment_train['sentiment'] = treatment_train['sentiment'].astype('category')\n",
    "exp_test['sentiment'] = exp_test['sentiment'].astype('category')\n",
    "\n",
    "print(\"# trainging instances in control group:\", control_train.shape[0])\n",
    "print(\"# trainging instances in treatment group:\", treatment_train.shape[0])\n",
    "print(\"# testing instances:\", exp_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "control_train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "treatment_train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (Optional) Write to file\n",
    "\n",
    "control_train.to_csv('data/control_train.csv', index=False)\n",
    "treatment_train.to_csv('data/treatment_train.csv', index=False)\n",
    "exp_test.to_csv('data/exp_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize vectorizers\n",
    "\n",
    "# baseline: unigram + bigram (binary)\n",
    "unigram_bigram_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "\n",
    "# unigram + bigram + stopwords removal\n",
    "unigram_bigram_noStopword_v = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=5, binary=True)\n",
    "\n",
    "# unigram + bigram (count)\n",
    "unigram_bigram_count_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "\n",
    "# unigram + bigram (tf-idf)\n",
    "unigram_bigram_tfidf_v = TfidfVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "\n",
    "# unigram + bigram + stemming\n",
    "unigram_bigram_stem_v = CountVectorizer(tokenizer=PorterStemmerTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "\n",
    "# unigram + bigram + lemmatization\n",
    "unigram_bigram_lemma_v = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate models on the test set\n",
    "\n",
    "vectorizers = [unigram_bigram_v, \n",
    "               unigram_bigram_noStopword_v, unigram_bigram_count_v, unigram_bigram_tfidf_v, \n",
    "               unigram_bigram_stem_v, unigram_bigram_lemma_v]\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit_control = evaluate_on_test_set(vectorizers, logit, control_train, exp_test)\n",
    "logit_treatment = evaluate_on_test_set(vectorizers, logit, treatment_train, exp_test)\n",
    "\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=1000, random_state=1)\n",
    "svm_control = evaluate_on_test_set(vectorizers, svm, control_train, exp_test)\n",
    "svm_treatment = evaluate_on_test_set(vectorizers, svm, treatment_train, exp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display results in dataframe and highlight the maximum in each column\n",
    "\n",
    "df = pd.DataFrame([logit_control, logit_treatment, svm_control, svm_treatment]).transpose()\n",
    "df.columns = ['logit_control', 'logit_treatment', 'svm_control', 'svm_treatment']\n",
    "df.index = ['unigram_bigram', \n",
    "            'unigram_bigram_noStopword', 'unigram_bigram_count', 'unigram_bigram_tfidf', \n",
    "            'unigram_bigram_stem', 'unigram_bigram_lemma']\n",
    "\n",
    "df.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results are very different each time. The additional ~400 negative examples seem to improve the performace in general but the best classifier does not always need them. Again the improvement is not consistent enough to be conclusive. \n",
    "\n",
    "If we adopt the second approach of using CWP samples as the test set, kappa boosts significantly (~0.97). I think that's an over-estimate because random sampling contains lots of template comments which are relatively easy to classify and less meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimize using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a pipeline and perform grid search\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2), min_df=5, binary=True)),\n",
    "                 ('clf', SGDClassifier(loss='hinge', penalty='l2', random_state=1))])\n",
    "\n",
    "parameters = {'vect__tokenizer': (PorterStemmerTokenizer(), LemmaTokenizer()),\n",
    "              'vect__stop_words': (None, 'english'), \n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "              'clf__n_iter': (200, 500, 1000),\n",
    "              'clf__class_weight': (None, 'balanced')}\n",
    "\n",
    "grid = GridSearchCV(pipe, parameters, scoring=make_scorer(cohen_kappa_score), n_jobs=-1)\n",
    "grid = grid.fit(labelledComments['comment'], labelledComments['sentiment'].cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 'A national monument does not need 1.35 MILLION acres!!'\n",
    "labelledComments.sentiment.cat.categories[grid.predict([n])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 'Secretary Zinke, I want you to know that I greatly support the Bears Ears National monument. I live in Northern Arizona and feel the wide open spaces of Southern Utah are very special. I enjoy hiking and camping in this area and would like to see it preserved for myself and future generations. Please keep Bears Ears protected!!'\n",
    "labelledComments.sentiment.cat.categories[grid.predict([p])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83561856545\n",
      "clf__alpha: 0.001\n",
      "clf__class_weight: 'balanced'\n",
      "clf__n_iter: 1000\n",
      "vect__stop_words: None\n",
      "vect__tokenizer: <__main__.PorterStemmerTokenizer object at 0x126eae198>\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)                                  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, grid.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# comments: 246733\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOI-2017-0002-0002</td>\n",
       "      <td>Our national monuments are a national treasure...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DOI-2017-0002-0003</td>\n",
       "      <td>1.We do not want National Monument protection ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DOI-2017-0002-0004</td>\n",
       "      <td>The monuments must be preserved. the precedent...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DOI-2017-0002-0005</td>\n",
       "      <td>My name is Ryan Erik Benally and I'm from Mont...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOI-2017-0002-0006</td>\n",
       "      <td>all protections and preservations for the enti...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          document_id                                            comment  \\\n",
       "0  DOI-2017-0002-0002  Our national monuments are a national treasure...   \n",
       "1  DOI-2017-0002-0003  1.We do not want National Monument protection ...   \n",
       "2  DOI-2017-0002-0004  The monuments must be preserved. the precedent...   \n",
       "3  DOI-2017-0002-0005  My name is Ryan Erik Benally and I'm from Mont...   \n",
       "4  DOI-2017-0002-0006  all protections and preservations for the enti...   \n",
       "\n",
       "  duplicate  \n",
       "0     False  \n",
       "1      True  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv(\"data/comments-cleaned.csv\")\n",
    "print(\"# comments:\", comments.shape[0])\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>duplicate</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOI-2017-0002-0002</td>\n",
       "      <td>Our national monuments are a national treasure...</td>\n",
       "      <td>False</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DOI-2017-0002-0003</td>\n",
       "      <td>1.We do not want National Monument protection ...</td>\n",
       "      <td>True</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DOI-2017-0002-0004</td>\n",
       "      <td>The monuments must be preserved. the precedent...</td>\n",
       "      <td>False</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DOI-2017-0002-0005</td>\n",
       "      <td>My name is Ryan Erik Benally and I'm from Mont...</td>\n",
       "      <td>False</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOI-2017-0002-0006</td>\n",
       "      <td>all protections and preservations for the enti...</td>\n",
       "      <td>False</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          document_id                                            comment  \\\n",
       "0  DOI-2017-0002-0002  Our national monuments are a national treasure...   \n",
       "1  DOI-2017-0002-0003  1.We do not want National Monument protection ...   \n",
       "2  DOI-2017-0002-0004  The monuments must be preserved. the precedent...   \n",
       "3  DOI-2017-0002-0005  My name is Ryan Erik Benally and I'm from Mont...   \n",
       "4  DOI-2017-0002-0006  all protections and preservations for the enti...   \n",
       "\n",
       "  duplicate prediction  \n",
       "0     False        pos  \n",
       "1      True        pos  \n",
       "2     False        pos  \n",
       "3     False        neg  \n",
       "4     False        pos  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['sentiment'] = grid.predict(comments.comment)\n",
    "comments = comments.replace({'sentiment': {2: 'pos', 1: 'neu', 0: 'neg'}})\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments.to_csv('data/comments-labelled-complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
