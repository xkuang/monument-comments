{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook:\n",
    "- splits labelled data into train, validation, and test sets\n",
    "- extract features and evaluation performance using 10-fold cross validation\n",
    "- optimize select model with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in the data from CSV\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "labelledComments = pd.read_csv(\"data/comments-to-label.csv\", usecols=['document_id', 'comment', 'sentiment']).dropna()\n",
    "labelledComments['comment'] = labelledComments['comment'].map(lambda x: re.sub(\"\\r\", \" \", x))\n",
    "\n",
    "print(\"# comments labelled:\", labelledComments.shape[0])\n",
    "labelledComments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View class distribution\n",
    "\n",
    "labelledComments['sentiment'] = labelledComments['sentiment'].astype('category')\n",
    "print(labelledComments.sentiment.value_counts())\n",
    "print(\"Index for class labels:\", labelledComments.sentiment.cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split labelled dataset into train (.7), validation (.2), and test (.1) sets \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train, validate, test = np.split(labelledComments.sample(frac=1), [int(.7*len(labelledComments)), int(.9*len(labelledComments))])\n",
    "\n",
    "train.to_csv('data/comments-train.csv', index=False)\n",
    "validate.to_csv('data/comments-validate.csv', index=False)\n",
    "test.to_csv('data/comments-test.csv', index=False)\n",
    "\n",
    "print(\"# train:\", train.shape[0])\n",
    "print(\"# validate:\", validate.shape[0])\n",
    "print(\"# test:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Load data from file\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# train = pd.read_csv(\"data/comments-train.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "# validate = pd.read_csv(\"data/comments-validate.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "# test = pd.read_csv(\"data/comments-test.csv\", usecols=['document_id', 'comment', 'sentiment'])\n",
    "# print(\"# train:\", train.shape[0])\n",
    "# print(\"# validate:\", validate.shape[0])\n",
    "# print(\"# test:\", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "- baseline\n",
    "- content v.s. style test: stopwords removal, occurrence count, and tf-idf\n",
    "- token-level analysis: stemming and lemmatization\n",
    "- full text v.s. select sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a bag-of-word matrix \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = train['comment']\n",
    "\n",
    "# baseline: unigram + bigram (binary)\n",
    "unigram_bigram_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram = unigram_bigram_v.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords removal, occurrence count, and tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simply change the parameters of CountVectorizer and TfidfVectorizer \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# unigram + bigram + stopwords removal\n",
    "unigram_bigram_noStopword_v = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_noStopword = unigram_bigram_noStopword_v.fit_transform(docs)\n",
    "\n",
    "# unigram + bigram (count)\n",
    "unigram_bigram_count_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "unigram_bigram_count = unigram_bigram_count_v.fit_transform(docs)\n",
    "\n",
    "# unigram + bigram (tf-idf)\n",
    "unigram_bigram_tfidf_v = TfidfVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "unigram_bigram_tfidf = unigram_bigram_tfidf_v.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Customize tokenizers\n",
    "\n",
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "class PorterStemmerTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = gensim.utils.simple_preprocess(doc, deacc=True, min_len=2)\n",
    "        return [self.stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = gensim.utils.simple_preprocess(doc, deacc=True, min_len=2)\n",
    "        return [self.wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pass customized tokenizer objects to CountVectorizer\n",
    "\n",
    "# unigram + bigram + stemming\n",
    "unigram_bigram_stem_v = CountVectorizer(tokenizer=PorterStemmerTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_stem = unigram_bigram_stem_v.fit_transform(docs)\n",
    "\n",
    "# unigram + bigram + lemmatization\n",
    "unigram_bigram_lemma_v = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_lemma = unigram_bigram_lemma_v.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select sentences\n",
    "- split text into sentences\n",
    "- remove the signature line if there is any\n",
    "- extract the first three and last three sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Remove default letter opening, i.e. \"Dear Secretary Ryan Zinke, \"\n",
    "\n",
    "# opening = \"(Dear Secretary Ryan Zinke,|Dear Secretary Zinke,)\"\n",
    "# labelledComments['comment'] = labelledComments['comment'].map(lambda x: re.sub(opening, \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write functions to split sentences, remove signature and extract sentences\n",
    "\n",
    "import re\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    '''\n",
    "    Function to break text (astring) into sentences (a list of strings).\n",
    "    # Ref: https://stackoverflow.com/a/31505798\n",
    "    '''\n",
    "    caps = \"([A-Z])\"\n",
    "    digits = \"([0-9])\"\n",
    "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "    starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip())>1]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def remove_signature(sentences):\n",
    "    '''\n",
    "    Function to remove the signature line from comment sentences\n",
    "    '''\n",
    "    closing = ['thank you','thanks','sincerely','regards']\n",
    "    last_sent = sentences[-1].lower()\n",
    "    if any(term in last_sent for term in closing):\n",
    "        return sentences[:-1]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def select_sentences(text, first_n, last_m):\n",
    "    '''\n",
    "    Function to extract the first N and the last M sentences from text, excluding the signature line.\n",
    "    '''\n",
    "    sentences = split_into_sentences(text)\n",
    "    sentences = remove_signature(sentences)\n",
    "    return \" \".join(sentences[:first_n]+sentences[-last_m:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract first and last 3 sentences from text and transform them to vectors\n",
    "\n",
    "train['first3last3'] = train['comment'].map(lambda x: select_sentences(x,3,3))\n",
    "sents = train['first3last3'] \n",
    "\n",
    "# unigram + bigram (first and last 3 sentences)\n",
    "unigram_bigram_sents_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_sents = unigram_bigram_sents_v.fit_transform(sents)\n",
    "\n",
    "# unigram + bigram + stemming (first and last 3 sentences)\n",
    "unigram_bigram_sents_stem_v = CountVectorizer(tokenizer=PorterStemmerTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_sents_stem = unigram_bigram_sents_stem_v.fit_transform(sents)\n",
    "\n",
    "# unigram + bigram + lemmatization (first and last 3 sentences)\n",
    "unigram_bigram_sents_lemma_v = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "unigram_bigram_sents_lemma = unigram_bigram_sents_lemma_v.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate models with 10-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics.classification import cohen_kappa_score\n",
    "\n",
    "\n",
    "def run_CV(feature_matrices, labels, model_list, num_folds):\n",
    "    '''\n",
    "    Build a classifier with for each feature matrix and model, and evaluate with \n",
    "    a N-fold cross validation. The performance is measured with kappa statistic.\n",
    "    The output is a list of performance results per model. \n",
    "    '''\n",
    "    cv = KFold(num_folds)    \n",
    "    result = []\n",
    "    for model in model_list:\n",
    "        model_scores = []\n",
    "        for X in feature_matrices:\n",
    "            kappa_scores = []\n",
    "            for train_index, test_index in cv.split(X): # cv\n",
    "                model.fit(X[train_index], labels.iloc[train_index]) # fit a logit model to the data\n",
    "                ypred = model.predict(X[test_index]) # make predictions\n",
    "                kappa = cohen_kappa_score(labels.iloc[test_index], ypred) # get kappa score\n",
    "                kappa_scores.append(kappa)\n",
    "            model_scores.append(np.mean(kappa_scores))\n",
    "        result.append(model_scores)\n",
    "    return result\n",
    "\n",
    "\n",
    "logit = LogisticRegression()\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=400, random_state=1)\n",
    "labels = train['sentiment'].cat.codes\n",
    "\n",
    "matrices = [unigram_bigram, \n",
    "            unigram_bigram_noStopword, unigram_bigram_count, unigram_bigram_tfidf, \n",
    "            unigram_bigram_stem, unigram_bigram_lemma,\n",
    "            unigram_bigram_sents, unigram_bigram_sents_stem, unigram_bigram_sents_lemma]\n",
    "\n",
    "result = run_CV(matrices, labels, [logit,svm], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display results in dataframe\n",
    "\n",
    "df = pd.DataFrame(result).transpose()\n",
    "df.columns = ['logit', 'svm']\n",
    "df.index = ['unigram_bigram', \n",
    "            'unigram_bigram_noStopword', 'unigram_bigram_count', 'unigram_bigram_tfidf', \n",
    "            'unigram_bigram_stem', 'unigram_bigram_lemma',\n",
    "            'unigram_bigram_sents', 'unigram_bigram_sents_stem', 'unigram_bigram_sents_lemma']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose one model to examine the most predictive features per class\n",
    "\n",
    "def print_topN_features(vectorizer, model, class_labels, n):\n",
    "    \"\"\"\n",
    "    Print features with the highest coefficient values, per class\n",
    "    \"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        topN = np.argsort(model.coef_[i])[-n:]\n",
    "        print(\"%s: %s\" % (class_label,\", \".join(feature_names[j] for j in topN)))\n",
    "\n",
    "print_topN_features(unigram_bigram_stem_v, svm, ['neg', 'neu', 'pos'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with additional negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# comments labelled: 2500\n",
      "# comments relabelled: 414\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "labelledComments = pd.read_csv(\"data/comments-to-label.csv\", usecols=['document_id', 'comment', 'sentiment']).dropna()\n",
    "labelledComments['comment'] = labelledComments['comment'].map(lambda x: re.sub(\"\\r\", \" \", x))\n",
    "\n",
    "relabelled = pd.read_csv(\"data/relabelled.csv\", usecols=['document_id', 'comment', 'Actual']).dropna()\n",
    "relabelled.columns = ['document_id', 'comment', 'sentiment']\n",
    "relabelled['comment'] = relabelled['comment'].map(lambda x: re.sub(\"\\r\", \" \", x))\n",
    "relabelled = relabelled.replace({'sentiment': {'Positive': 'pos', 'Neutral': 'neu', 'Negative': 'neg'}})\n",
    "relabelled = relabelled.drop_duplicates(subset=['comment'])\n",
    "\n",
    "print(\"# comments labelled:\", labelledComments.shape[0])\n",
    "print(\"# comments relabelled:\", relabelled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trainging instances in control group: 2000\n",
      "# trainging instances in treatment group: 2414\n",
      "# testing instances: 489\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets for training and testing\n",
    "# Approach: use a subset of 2500 labelled data as test set\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# find overlapping comments\n",
    "labelled_id = set(labelledComments.document_id)\n",
    "relabelled_id = set(relabelled.document_id)\n",
    "eligible_id = labelled_id - relabelled_id\n",
    "\n",
    "# select eligible instances\n",
    "eligibleComments = labelledComments.loc[labelledComments['document_id'].isin(eligible_id)]\n",
    "\n",
    "# divide into training set (control group) and test set \n",
    "control_train, exp_test = np.split(eligibleComments.sample(frac=1), [int(.8*len(labelledComments))])\n",
    "\n",
    "# add relabelled instances to the train set to form treatment group\n",
    "treatment_train = control_train.append(relabelled, ignore_index=True)\n",
    "\n",
    "# change the data type of sentiment column\n",
    "control_train['sentiment'] = control_train['sentiment'].astype('category')\n",
    "treatment_train['sentiment'] = treatment_train['sentiment'].astype('category')\n",
    "exp_test['sentiment'] = exp_test['sentiment'].astype('category')\n",
    "\n",
    "print(\"# trainging instances in control group:\", control_train.shape[0])\n",
    "print(\"# trainging instances in treatment group:\", treatment_train.shape[0])\n",
    "print(\"# testing instances:\", exp_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Another approach: using CWP samples as test set\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# cwp = pd.read_csv(\"data/cwp-sample.csv\", usecols=['ID', 'comment', 'Sentiment']).dropna()\n",
    "# cwp.columns = ['document_id', 'comment', 'sentiment']\n",
    "# cwp['comment'] = cwp['comment'].map(lambda x: re.sub(\"\\r\", \" \", x))\n",
    "# cwp = cwp.replace({'sentiment': {'Positive': 'pos', 'Neutral': 'neu', 'Negative': 'neg'}})\n",
    "# cwp['first50words'] = cwp['comment'].map(lambda x: x[:50])\n",
    "# exp_test = cwp.drop_duplicates(subset=['first50words']) # test set\n",
    "\n",
    "# # find overlapping comments\n",
    "# labelled_id = set(labelledComments.document_id)\n",
    "# relabelled_id = set(relabelled.document_id)\n",
    "# test_id = set(exp_test.document_id)\n",
    "\n",
    "# # define train set (control group) and test set\n",
    "# control_train_id = labelled_id - relabelled_id - test_id\n",
    "# control_train = labelledComments.loc[labelledComments['document_id'].isin(eligible_id)]\n",
    "\n",
    "# # add relabelled instances to the train set to form treatment group\n",
    "# extra_id = relabelled_id - test_id\n",
    "# extra = relabelled.loc[relabelled['document_id'].isin(extra_id)]\n",
    "# treatment_train = control_train.append(extra, ignore_index=True)\n",
    "\n",
    "# # change the data type of sentiment column\n",
    "# control_train['sentiment'] = control_train['sentiment'].astype('category')\n",
    "# treatment_train['sentiment'] = treatment_train['sentiment'].astype('category')\n",
    "# exp_test['sentiment'] = exp_test['sentiment'].astype('category')\n",
    "\n",
    "# print(\"# trainging instances in control group:\", control_train.shape[0])\n",
    "# print(\"# trainging instances in treatment group:\", treatment_train.shape[0])\n",
    "# print(\"# testing instances:\", exp_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (Optional) Write to file\n",
    "\n",
    "control_train.to_csv('data/control_train.csv', index=False)\n",
    "treatment_train.to_csv('data/treatment_train.csv', index=False)\n",
    "exp_test.to_csv('data/exp_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Customize tokenizers\n",
    "\n",
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "class PorterStemmerTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = gensim.utils.simple_preprocess(doc, deacc=True, min_len=2)\n",
    "        return [self.stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = gensim.utils.simple_preprocess(doc, deacc=True, min_len=2)\n",
    "        return [self.wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize vectorizers\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# baseline: unigram + bigram (binary)\n",
    "unigram_bigram_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "\n",
    "# unigram + bigram + stopwords removal\n",
    "unigram_bigram_noStopword_v = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=5, binary=True)\n",
    "\n",
    "# unigram + bigram (count)\n",
    "unigram_bigram_count_v = CountVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "\n",
    "# unigram + bigram (tf-idf)\n",
    "unigram_bigram_tfidf_v = TfidfVectorizer(stop_words=None, ngram_range=(1, 2), min_df=5, binary=False)\n",
    "\n",
    "# unigram + bigram + stemming\n",
    "unigram_bigram_stem_v = CountVectorizer(tokenizer=PorterStemmerTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)\n",
    "\n",
    "# unigram + bigram + lemmatization\n",
    "unigram_bigram_lemma_v = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=None, ngram_range=(1, 2), min_df=5, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58214651931183781, 0.5014393666786614, 0.47071892446166119, 0.070624904971871683, 0.61953984661553851, 0.54283043122589703]\n",
      "[0.67819175863116321, 0.64554209381795591, 0.61255818559968311, 0.45780460761365038, 0.70941863919976234, 0.63727879622761474]\n",
      "[0.62100857703833845, 0.57383557664374929, 0.48346889194042464, 0.25673621666436375, 0.75245519894704871, 0.64590876176683565]\n",
      "[0.69191625472521079, 0.5883430721996632, 0.6445187554521663, 0.5014393666786614, 0.76796052007212678, 0.67819175863116321]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models on the test set\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics.classification import cohen_kappa_score\n",
    "\n",
    "\n",
    "def evaluate_on_test_set(vectorizers, model, train_data, test_data):\n",
    "    scores = []\n",
    "    for X in vectorizers:\n",
    "        train_vectors = X.fit_transform(train_data['comment']) # construct feature vectors (train)\n",
    "        clf = model.fit(train_vectors, train_data['sentiment'].cat.codes) # fit model\n",
    "        test_vectors = X.transform(test_data['comment']) # construct feature vectors (test)\n",
    "        predicted = clf.predict(test_vectors) # make predictions\n",
    "        kappa = cohen_kappa_score(test_data['sentiment'].cat.codes, predicted) # get kappa score\n",
    "        scores.append(kappa)\n",
    "    print(scores)\n",
    "    return scores\n",
    "\n",
    "\n",
    "vectorizers = [unigram_bigram_v, \n",
    "               unigram_bigram_noStopword_v, unigram_bigram_count_v, unigram_bigram_tfidf_v, \n",
    "               unigram_bigram_stem_v, unigram_bigram_lemma_v]\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit_control = evaluate_on_test_set(vectorizers, logit, control_train, exp_test)\n",
    "logit_treatment = evaluate_on_test_set(vectorizers, logit, treatment_train, exp_test)\n",
    "\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=400, random_state=1)\n",
    "svm_control = evaluate_on_test_set(vectorizers, svm, control_train, exp_test)\n",
    "svm_treatment = evaluate_on_test_set(vectorizers, svm, treatment_train, exp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display results in dataframe\n",
    "\n",
    "df = pd.DataFrame([logit_control, logit_treatment, svm_control, svm_treatment]).transpose()\n",
    "df.columns = ['logit_control', 'logit_treatment', 'svm_control', 'svm_treatment']\n",
    "df.index = ['unigram_bigram', \n",
    "            'unigram_bigram_noStopword', 'unigram_bigram_count', 'unigram_bigram_tfidf', \n",
    "            'unigram_bigram_stem', 'unigram_bigram_lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style  type=\"text/css\" >\n",
       "        \n",
       "        \n",
       "            #T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow4_col0 {\n",
       "            \n",
       "                background-color:  yellow;\n",
       "            \n",
       "            }\n",
       "        \n",
       "            #T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow4_col1 {\n",
       "            \n",
       "                background-color:  yellow;\n",
       "            \n",
       "            }\n",
       "        \n",
       "            #T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow4_col2 {\n",
       "            \n",
       "                background-color:  yellow;\n",
       "            \n",
       "            }\n",
       "        \n",
       "            #T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow4_col3 {\n",
       "            \n",
       "                background-color:  yellow;\n",
       "            \n",
       "            }\n",
       "        \n",
       "        </style>\n",
       "\n",
       "        <table id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175d\" None>\n",
       "        \n",
       "\n",
       "        <thead>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th class=\"blank\">\n",
       "                \n",
       "                <th class=\"col_heading level0 col0\">logit_control\n",
       "                \n",
       "                <th class=\"col_heading level0 col1\">logit_treatment\n",
       "                \n",
       "                <th class=\"col_heading level0 col2\">svm_control\n",
       "                \n",
       "                <th class=\"col_heading level0 col3\">svm_treatment\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "        </thead>\n",
       "        <tbody>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175d\" class=\"row_heading level3 row0\">\n",
       "                    unigram_bigram\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow0_col0\" class=\"data row0 col0\">\n",
       "                    0.582147\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow0_col1\" class=\"data row0 col1\">\n",
       "                    0.678192\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow0_col2\" class=\"data row0 col2\">\n",
       "                    0.621009\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow0_col3\" class=\"data row0 col3\">\n",
       "                    0.691916\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175d\" class=\"row_heading level3 row1\">\n",
       "                    unigram_bigram_noStopword\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow1_col0\" class=\"data row1 col0\">\n",
       "                    0.501439\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow1_col1\" class=\"data row1 col1\">\n",
       "                    0.645542\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow1_col2\" class=\"data row1 col2\">\n",
       "                    0.573836\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow1_col3\" class=\"data row1 col3\">\n",
       "                    0.588343\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175d\" class=\"row_heading level3 row2\">\n",
       "                    unigram_bigram_count\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow2_col0\" class=\"data row2 col0\">\n",
       "                    0.470719\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow2_col1\" class=\"data row2 col1\">\n",
       "                    0.612558\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow2_col2\" class=\"data row2 col2\">\n",
       "                    0.483469\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow2_col3\" class=\"data row2 col3\">\n",
       "                    0.644519\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175d\" class=\"row_heading level3 row3\">\n",
       "                    unigram_bigram_tfidf\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow3_col0\" class=\"data row3 col0\">\n",
       "                    0.0706249\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow3_col1\" class=\"data row3 col1\">\n",
       "                    0.457805\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow3_col2\" class=\"data row3 col2\">\n",
       "                    0.256736\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow3_col3\" class=\"data row3 col3\">\n",
       "                    0.501439\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175d\" class=\"row_heading level3 row4\">\n",
       "                    unigram_bigram_stem\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow4_col0\" class=\"data row4 col0\">\n",
       "                    0.61954\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow4_col1\" class=\"data row4 col1\">\n",
       "                    0.709419\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow4_col2\" class=\"data row4 col2\">\n",
       "                    0.752455\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow4_col3\" class=\"data row4 col3\">\n",
       "                    0.767961\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "                \n",
       "                <th id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175d\" class=\"row_heading level3 row5\">\n",
       "                    unigram_bigram_lemma\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow5_col0\" class=\"data row5 col0\">\n",
       "                    0.54283\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow5_col1\" class=\"data row5 col1\">\n",
       "                    0.637279\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow5_col2\" class=\"data row5 col2\">\n",
       "                    0.645909\n",
       "                \n",
       "                <td id=\"T_e2a356d4_5e1b_11e7_86b5_f45c89b9175drow5_col3\" class=\"data row5 col3\">\n",
       "                    0.678192\n",
       "                \n",
       "            </tr>\n",
       "            \n",
       "        </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<pandas.formats.style.Styler at 0x113b0eb70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Highlight the maximum in each column\n",
    "\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "df.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results are very different each time. The additional ~400 negative examples seem to improve the performace in general but the best classifier does not always need them. Again the improvement is not consistent enough to be conclusive. \n",
    "\n",
    "If we adopt the second approach of using CWP samples as the test set, kappa boosts significantly (~0.97). I think that's an over-estimate because random sampling contains lots of template comments which are relatively easy to classify and less meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimize\n",
    "- introduce additional features\n",
    "- adjust class weights\n",
    "- tune parameter using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
